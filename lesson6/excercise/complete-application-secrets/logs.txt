
==> Audit <==
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COMMAND ‚îÇ            ARGS            ‚îÇ PROFILE  ‚îÇ USER  ‚îÇ VERSION ‚îÇ      START TIME       ‚îÇ       END TIME        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ service ‚îÇ nginx-nodeport             ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 09 Dec 25 11:31 +0545 ‚îÇ 09 Dec 25 11:31 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 09 Dec 25 11:32 +0545 ‚îÇ 09 Dec 25 11:32 +0545 ‚îÇ
‚îÇ tunnel  ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 09 Dec 25 11:35 +0545 ‚îÇ 09 Dec 25 11:41 +0545 ‚îÇ
‚îÇ service ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 10:21 +0545 ‚îÇ                       ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 10:21 +0545 ‚îÇ 11 Dec 25 10:21 +0545 ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 10:21 +0545 ‚îÇ 11 Dec 25 10:22 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 10:22 +0545 ‚îÇ 11 Dec 25 10:22 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress             ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 11:57 +0545 ‚îÇ 11 Dec 25 11:57 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 12:07 +0545 ‚îÇ 11 Dec 25 12:07 +0545 ‚îÇ
‚îÇ tunnel  ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 11 Dec 25 12:10 +0545 ‚îÇ                       ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:26 +0545 ‚îÇ                       ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:27 +0545 ‚îÇ 12 Dec 25 10:27 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:27 +0545 ‚îÇ 12 Dec 25 10:27 +0545 ‚îÇ
‚îÇ tunnel  ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:30 +0545 ‚îÇ 12 Dec 25 10:30 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:42 +0545 ‚îÇ 12 Dec 25 10:42 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:42 +0545 ‚îÇ 12 Dec 25 10:42 +0545 ‚îÇ
‚îÇ tunnel  ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:43 +0545 ‚îÇ                       ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 10:44 +0545 ‚îÇ 12 Dec 25 10:44 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 11:00 +0545 ‚îÇ 12 Dec 25 11:00 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 12 Dec 25 11:03 +0545 ‚îÇ 12 Dec 25 11:03 +0545 ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 14 Dec 25 14:07 +0545 ‚îÇ 14 Dec 25 14:07 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 14 Dec 25 14:07 +0545 ‚îÇ 14 Dec 25 14:07 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 16 Dec 25 12:04 +0545 ‚îÇ                       ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 16 Dec 25 12:04 +0545 ‚îÇ 16 Dec 25 12:04 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 17 Dec 25 10:28 +0545 ‚îÇ                       ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 17 Dec 25 10:28 +0545 ‚îÇ 17 Dec 25 10:28 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 18 Dec 25 11:20 +0545 ‚îÇ                       ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 18 Dec 25 11:20 +0545 ‚îÇ 18 Dec 25 11:20 +0545 ‚îÇ
‚îÇ ip      ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 11:49 +0545 ‚îÇ                       ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 11:49 +0545 ‚îÇ 19 Dec 25 11:49 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable volumesnapshots     ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 14:13 +0545 ‚îÇ 19 Dec 25 14:13 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 14:13 +0545 ‚îÇ                       ‚îÇ
‚îÇ addons  ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 16:38 +0545 ‚îÇ 19 Dec 25 16:38 +0545 ‚îÇ
‚îÇ addons  ‚îÇ list                       ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 16:38 +0545 ‚îÇ 19 Dec 25 16:38 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 19 Dec 25 16:40 +0545 ‚îÇ 19 Dec 25 16:41 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 11:02 +0545 ‚îÇ 22 Dec 25 11:02 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable volumesnapshots     ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:07 +0545 ‚îÇ 22 Dec 25 12:07 +0545 ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:08 +0545 ‚îÇ 22 Dec 25 12:08 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:08 +0545 ‚îÇ 22 Dec 25 12:08 +0545 ‚îÇ
‚îÇ delete  ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:19 +0545 ‚îÇ 22 Dec 25 12:19 +0545 ‚îÇ
‚îÇ start   ‚îÇ --driver=docker            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:20 +0545 ‚îÇ 22 Dec 25 12:21 +0545 ‚îÇ
‚îÇ ssh     ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:45 +0545 ‚îÇ 22 Dec 25 12:45 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 12:59 +0545 ‚îÇ                       ‚îÇ
‚îÇ addons  ‚îÇ enable volumesnapshots     ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:00 +0545 ‚îÇ 22 Dec 25 13:00 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:00 +0545 ‚îÇ                       ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:02 +0545 ‚îÇ                       ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:05 +0545 ‚îÇ                       ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:05 +0545 ‚îÇ                       ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:13 +0545 ‚îÇ 22 Dec 25 13:13 +0545 ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:13 +0545 ‚îÇ 22 Dec 25 13:13 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:13 +0545 ‚îÇ 22 Dec 25 13:14 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable csi-hostpath-driver ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:14 +0545 ‚îÇ 22 Dec 25 13:19 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable volumesnapshots     ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 22 Dec 25 13:19 +0545 ‚îÇ 22 Dec 25 13:19 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 27 Dec 25 13:26 +0545 ‚îÇ 27 Dec 25 13:26 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 29 Dec 25 19:50 +0545 ‚îÇ 29 Dec 25 19:50 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 30 Dec 25 19:20 +0545 ‚îÇ 30 Dec 25 19:21 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress             ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 30 Dec 25 19:34 +0545 ‚îÇ                       ‚îÇ
‚îÇ stop    ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 30 Dec 25 19:38 +0545 ‚îÇ 30 Dec 25 19:38 +0545 ‚îÇ
‚îÇ start   ‚îÇ                            ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 30 Dec 25 19:39 +0545 ‚îÇ 30 Dec 25 19:39 +0545 ‚îÇ
‚îÇ addons  ‚îÇ enable ingress             ‚îÇ minikube ‚îÇ aarav ‚îÇ v1.37.0 ‚îÇ 30 Dec 25 19:39 +0545 ‚îÇ                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò


==> Last Start <==
Log file created at: 2025/12/30 19:39:20
Running on machine: aarav-Nitro-AN515-58
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1230 19:39:20.947051 1664722 out.go:360] Setting OutFile to fd 1 ...
I1230 19:39:20.947192 1664722 out.go:413] isatty.IsTerminal(1) = true
I1230 19:39:20.947194 1664722 out.go:374] Setting ErrFile to fd 2...
I1230 19:39:20.947196 1664722 out.go:413] isatty.IsTerminal(2) = true
I1230 19:39:20.947320 1664722 root.go:338] Updating PATH: /home/aarav/.minikube/bin
I1230 19:39:20.947559 1664722 out.go:368] Setting JSON to false
I1230 19:39:20.948748 1664722 start.go:130] hostinfo: {"hostname":"aarav-Nitro-AN515-58","uptime":283356,"bootTime":1766819505,"procs":456,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.8.0-90-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"3290551c-4cf2-4fcd-a254-4c88231723a0"}
I1230 19:39:20.948793 1664722 start.go:140] virtualization: kvm host
I1230 19:39:20.954411 1664722 out.go:179] üòÑ  minikube v1.37.0 on Ubuntu 22.04
I1230 19:39:20.967806 1664722 notify.go:220] Checking for updates...
I1230 19:39:20.968004 1664722 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1230 19:39:20.968081 1664722 driver.go:421] Setting default libvirt URI to qemu:///system
I1230 19:39:21.006489 1664722 docker.go:123] docker version: linux-26.1.1:Docker Desktop 4.30.0 (149282)
I1230 19:39:21.006549 1664722 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1230 19:39:21.083114 1664722 info.go:266] docker info: {ID:92aae403-c08a-45c1-b363-de228b93fc56 Containers:58 ContainersRunning:0 ContainersPaused:0 ContainersStopped:58 Images:101 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:54 OomKillDisable:false NGoroutines:79 SystemTime:2025-12-30 13:54:21.074107697 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3917635584 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///home/aarav/.docker/desktop/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/usr/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1230 19:39:21.083184 1664722 docker.go:318] overlay module found
I1230 19:39:21.088602 1664722 out.go:179] ‚ú®  Using the docker driver based on existing profile
I1230 19:39:21.093857 1664722 start.go:304] selected driver: docker
I1230 19:39:21.093861 1664722 start.go:918] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3688 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[csi-hostpath-driver:true default-storageclass:true storage-provisioner:true volumesnapshots:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1230 19:39:21.093910 1664722 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1230 19:39:21.093996 1664722 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1230 19:39:21.172806 1664722 info.go:266] docker info: {ID:92aae403-c08a-45c1-b363-de228b93fc56 Containers:58 ContainersRunning:0 ContainersPaused:0 ContainersStopped:58 Images:101 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:54 OomKillDisable:false NGoroutines:79 SystemTime:2025-12-30 13:54:21.163836723 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:6.6.26-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:3917635584 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///home/aarav/.docker/desktop/docker-cli.sock] ExperimentalBuild:false ServerVersion:26.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:e377cd56a71523140ca6ae87e30244719194a521 Expected:e377cd56a71523140ca6ae87e30244719194a521} RuncCommit:{ID:v1.1.12-0-g51d5e94 Expected:v1.1.12-0-g51d5e94} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/lib/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-buildx] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0-desktop.1] map[Name:compose Path:/usr/lib/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShadowedPaths:[/usr/libexec/docker/cli-plugins/docker-compose] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.27.0-desktop.2] map[Name:debug Path:/usr/lib/docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.29] map[Name:dev Path:/usr/lib/docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:/usr/lib/docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.23] map[Name:feedback Path:/usr/lib/docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.4] map[Name:init Path:/usr/lib/docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.1.0] map[Name:sbom Path:/usr/lib/docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/usr/lib/docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.8.0]] Warnings:<nil>}}
I1230 19:39:21.173381 1664722 cni.go:84] Creating CNI manager for ""
I1230 19:39:21.173413 1664722 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1230 19:39:21.173438 1664722 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3688 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[csi-hostpath-driver:true default-storageclass:true storage-provisioner:true volumesnapshots:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1230 19:39:21.178865 1664722 out.go:179] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I1230 19:39:21.189368 1664722 cache.go:123] Beginning downloading kic base image for docker with docker
I1230 19:39:21.194786 1664722 out.go:179] üöú  Pulling base image v0.0.48 ...
I1230 19:39:21.205540 1664722 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1230 19:39:21.205599 1664722 preload.go:146] Found local preload: /home/aarav/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1230 19:39:21.205606 1664722 cache.go:58] Caching tarball of preloaded images
I1230 19:39:21.205626 1664722 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1230 19:39:21.205702 1664722 preload.go:172] Found /home/aarav/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1230 19:39:21.205719 1664722 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1230 19:39:21.205824 1664722 profile.go:143] Saving config to /home/aarav/.minikube/profiles/minikube/config.json ...
I1230 19:39:21.259028 1664722 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon, skipping pull
I1230 19:39:21.259036 1664722 cache.go:147] gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 exists in daemon, skipping load
I1230 19:39:21.259044 1664722 cache.go:232] Successfully downloaded all kic artifacts
I1230 19:39:21.259058 1664722 start.go:360] acquireMachinesLock for minikube: {Name:mk96ace255964baa039e523204b3d13eeb1bcbb6 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1230 19:39:21.259093 1664722 start.go:364] duration metric: took 24.683¬µs to acquireMachinesLock for "minikube"
I1230 19:39:21.259111 1664722 start.go:96] Skipping create...Using existing machine configuration
I1230 19:39:21.259114 1664722 fix.go:54] fixHost starting: 
I1230 19:39:21.259266 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:21.293957 1664722 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1230 19:39:21.293972 1664722 fix.go:138] unexpected machine state, will restart: <nil>
I1230 19:39:21.304651 1664722 out.go:252] üîÑ  Restarting existing docker container for "minikube" ...
I1230 19:39:21.304754 1664722 cli_runner.go:164] Run: docker start minikube
I1230 19:39:21.627214 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:21.675185 1664722 kic.go:430] container "minikube" state is running.
I1230 19:39:21.675700 1664722 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1230 19:39:21.716570 1664722 profile.go:143] Saving config to /home/aarav/.minikube/profiles/minikube/config.json ...
I1230 19:39:21.716733 1664722 machine.go:93] provisionDockerMachine start ...
I1230 19:39:21.716788 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:21.757963 1664722 main.go:141] libmachine: Using SSH client type: native
I1230 19:39:21.758158 1664722 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 37117 <nil> <nil>}
I1230 19:39:21.758164 1664722 main.go:141] libmachine: About to run SSH command:
hostname
I1230 19:39:21.759144 1664722 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1230 19:39:24.917697 1664722 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1230 19:39:24.917734 1664722 ubuntu.go:182] provisioning hostname "minikube"
I1230 19:39:24.917859 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:24.959034 1664722 main.go:141] libmachine: Using SSH client type: native
I1230 19:39:24.959159 1664722 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 37117 <nil> <nil>}
I1230 19:39:24.959164 1664722 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1230 19:39:25.115584 1664722 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1230 19:39:25.115636 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:25.154737 1664722 main.go:141] libmachine: Using SSH client type: native
I1230 19:39:25.154956 1664722 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 37117 <nil> <nil>}
I1230 19:39:25.154969 1664722 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1230 19:39:25.300343 1664722 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1230 19:39:25.300363 1664722 ubuntu.go:188] set auth options {CertDir:/home/aarav/.minikube CaCertPath:/home/aarav/.minikube/certs/ca.pem CaPrivateKeyPath:/home/aarav/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/aarav/.minikube/machines/server.pem ServerKeyPath:/home/aarav/.minikube/machines/server-key.pem ClientKeyPath:/home/aarav/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/aarav/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/aarav/.minikube}
I1230 19:39:25.300397 1664722 ubuntu.go:190] setting up certificates
I1230 19:39:25.300407 1664722 provision.go:84] configureAuth start
I1230 19:39:25.300514 1664722 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1230 19:39:25.337894 1664722 provision.go:143] copyHostCerts
I1230 19:39:25.337921 1664722 exec_runner.go:144] found /home/aarav/.minikube/ca.pem, removing ...
I1230 19:39:25.337930 1664722 exec_runner.go:203] rm: /home/aarav/.minikube/ca.pem
I1230 19:39:25.337968 1664722 exec_runner.go:151] cp: /home/aarav/.minikube/certs/ca.pem --> /home/aarav/.minikube/ca.pem (1074 bytes)
I1230 19:39:25.338047 1664722 exec_runner.go:144] found /home/aarav/.minikube/cert.pem, removing ...
I1230 19:39:25.338049 1664722 exec_runner.go:203] rm: /home/aarav/.minikube/cert.pem
I1230 19:39:25.338062 1664722 exec_runner.go:151] cp: /home/aarav/.minikube/certs/cert.pem --> /home/aarav/.minikube/cert.pem (1119 bytes)
I1230 19:39:25.338086 1664722 exec_runner.go:144] found /home/aarav/.minikube/key.pem, removing ...
I1230 19:39:25.338087 1664722 exec_runner.go:203] rm: /home/aarav/.minikube/key.pem
I1230 19:39:25.338096 1664722 exec_runner.go:151] cp: /home/aarav/.minikube/certs/key.pem --> /home/aarav/.minikube/key.pem (1675 bytes)
I1230 19:39:25.338141 1664722 provision.go:117] generating server cert: /home/aarav/.minikube/machines/server.pem ca-key=/home/aarav/.minikube/certs/ca.pem private-key=/home/aarav/.minikube/certs/ca-key.pem org=aarav.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1230 19:39:25.450557 1664722 provision.go:177] copyRemoteCerts
I1230 19:39:25.450608 1664722 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1230 19:39:25.450640 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:25.484019 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:25.585070 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I1230 19:39:25.611750 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1230 19:39:25.633781 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1230 19:39:25.651797 1664722 provision.go:87] duration metric: took 351.380368ms to configureAuth
I1230 19:39:25.651811 1664722 ubuntu.go:206] setting minikube options for container-runtime
I1230 19:39:25.651964 1664722 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1230 19:39:25.652021 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:25.691041 1664722 main.go:141] libmachine: Using SSH client type: native
I1230 19:39:25.691276 1664722 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 37117 <nil> <nil>}
I1230 19:39:25.691282 1664722 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1230 19:39:25.841093 1664722 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1230 19:39:25.841108 1664722 ubuntu.go:71] root file system type: overlay
I1230 19:39:25.841253 1664722 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1230 19:39:25.841400 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:25.885073 1664722 main.go:141] libmachine: Using SSH client type: native
I1230 19:39:25.885203 1664722 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 37117 <nil> <nil>}
I1230 19:39:25.885242 1664722 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1230 19:39:26.034036 1664722 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1230 19:39:26.034141 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:26.072000 1664722 main.go:141] libmachine: Using SSH client type: native
I1230 19:39:26.072214 1664722 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 37117 <nil> <nil>}
I1230 19:39:26.072225 1664722 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1230 19:39:26.218629 1664722 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1230 19:39:26.218646 1664722 machine.go:96] duration metric: took 4.501906175s to provisionDockerMachine
I1230 19:39:26.218655 1664722 start.go:293] postStartSetup for "minikube" (driver="docker")
I1230 19:39:26.218665 1664722 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1230 19:39:26.218761 1664722 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1230 19:39:26.218816 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:26.259903 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:26.366650 1664722 ssh_runner.go:195] Run: cat /etc/os-release
I1230 19:39:26.371434 1664722 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1230 19:39:26.371453 1664722 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1230 19:39:26.371460 1664722 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1230 19:39:26.371465 1664722 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1230 19:39:26.371478 1664722 filesync.go:126] Scanning /home/aarav/.minikube/addons for local assets ...
I1230 19:39:26.371534 1664722 filesync.go:126] Scanning /home/aarav/.minikube/files for local assets ...
I1230 19:39:26.380392 1664722 start.go:296] duration metric: took 161.723053ms for postStartSetup
I1230 19:39:26.380515 1664722 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1230 19:39:26.380608 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:26.421398 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:26.515519 1664722 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1230 19:39:26.524220 1664722 fix.go:56] duration metric: took 5.265094595s for fixHost
I1230 19:39:26.524239 1664722 start.go:83] releasing machines lock for "minikube", held for 5.26513847s
I1230 19:39:26.524385 1664722 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1230 19:39:26.573480 1664722 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1230 19:39:26.573535 1664722 ssh_runner.go:195] Run: cat /version.json
I1230 19:39:26.573560 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:26.573630 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:26.617599 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:26.617929 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:28.527240 1664722 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.953729447s)
I1230 19:39:28.527345 1664722 ssh_runner.go:235] Completed: cat /version.json: (1.953786631s)
I1230 19:39:28.527580 1664722 ssh_runner.go:195] Run: systemctl --version
I1230 19:39:28.536596 1664722 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1230 19:39:28.543896 1664722 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1230 19:39:28.567048 1664722 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1230 19:39:28.567205 1664722 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1230 19:39:28.580366 1664722 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1230 19:39:28.580378 1664722 start.go:495] detecting cgroup driver to use...
I1230 19:39:28.580401 1664722 detect.go:190] detected "systemd" cgroup driver on host os
I1230 19:39:28.580467 1664722 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1230 19:39:28.597607 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1230 19:39:28.611960 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1230 19:39:28.622044 1664722 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1230 19:39:28.622128 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1230 19:39:28.631719 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1230 19:39:28.647211 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1230 19:39:28.659648 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1230 19:39:28.672055 1664722 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1230 19:39:28.682336 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1230 19:39:28.689521 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1230 19:39:28.696557 1664722 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1230 19:39:28.703219 1664722 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1230 19:39:28.709485 1664722 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1230 19:39:28.715085 1664722 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1230 19:39:28.756087 1664722 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1230 19:39:28.810997 1664722 start.go:495] detecting cgroup driver to use...
I1230 19:39:28.811045 1664722 detect.go:190] detected "systemd" cgroup driver on host os
I1230 19:39:28.811116 1664722 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1230 19:39:28.819556 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1230 19:39:28.826499 1664722 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1230 19:39:28.837258 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1230 19:39:28.844840 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1230 19:39:28.853101 1664722 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1230 19:39:28.865514 1664722 ssh_runner.go:195] Run: which cri-dockerd
I1230 19:39:28.868988 1664722 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1230 19:39:28.878563 1664722 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1230 19:39:28.895041 1664722 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1230 19:39:28.946823 1664722 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1230 19:39:28.990934 1664722 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1230 19:39:28.991014 1664722 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1230 19:39:29.002212 1664722 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1230 19:39:29.009176 1664722 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1230 19:39:29.052179 1664722 ssh_runner.go:195] Run: sudo systemctl restart docker
I1230 19:39:29.535650 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1230 19:39:29.545087 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1230 19:39:29.555250 1664722 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1230 19:39:29.563863 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1230 19:39:29.571218 1664722 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1230 19:39:29.612680 1664722 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1230 19:39:29.654694 1664722 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1230 19:39:29.694380 1664722 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1230 19:39:29.713473 1664722 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1230 19:39:29.721500 1664722 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1230 19:39:29.757561 1664722 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1230 19:39:29.831918 1664722 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1230 19:39:29.842151 1664722 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1230 19:39:29.842221 1664722 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1230 19:39:29.844921 1664722 start.go:563] Will wait 60s for crictl version
I1230 19:39:29.844974 1664722 ssh_runner.go:195] Run: which crictl
I1230 19:39:29.847386 1664722 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1230 19:39:29.871689 1664722 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1230 19:39:29.871746 1664722 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1230 19:39:29.888543 1664722 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1230 19:39:29.909123 1664722 out.go:252] üê≥  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1230 19:39:29.909221 1664722 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1230 19:39:29.947665 1664722 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1230 19:39:29.950823 1664722 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1230 19:39:29.959015 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1230 19:39:29.994211 1664722 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3688 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[csi-hostpath-driver:true default-storageclass:true storage-provisioner:true volumesnapshots:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1230 19:39:29.994287 1664722 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1230 19:39:29.994347 1664722 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1230 19:39:30.011021 1664722 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
nginx:1.26
busybox:latest
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
hashicorp/http-echo:latest
openebs/node-disk-exporter:2.1.0
openebs/node-disk-manager:2.1.0
openebs/node-disk-operator:2.1.0
registry.k8s.io/sig-storage/livenessprobe:<none>
registry.k8s.io/sig-storage/csi-node-driver-registrar:<none>
openebs/provisioner-localpv:3.4.0
registry.k8s.io/sig-storage/csi-provisioner:<none>
registry.k8s.io/sig-storage/csi-snapshotter:<none>
registry.k8s.io/sig-storage/snapshot-controller:<none>
registry.k8s.io/sig-storage/csi-external-health-monitor-controller:<none>
registry.k8s.io/sig-storage/csi-attacher:<none>
registry.k8s.io/sig-storage/csi-resizer:<none>
registry.k8s.io/sig-storage/hostpathplugin:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1230 19:39:30.011028 1664722 docker.go:621] Images already preloaded, skipping extraction
I1230 19:39:30.011072 1664722 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1230 19:39:30.030141 1664722 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
nginx:1.26
busybox:latest
registry.k8s.io/ingress-nginx/kube-webhook-certgen:<none>
hashicorp/http-echo:latest
openebs/node-disk-exporter:2.1.0
openebs/node-disk-manager:2.1.0
openebs/node-disk-operator:2.1.0
registry.k8s.io/sig-storage/livenessprobe:<none>
registry.k8s.io/sig-storage/csi-node-driver-registrar:<none>
openebs/provisioner-localpv:3.4.0
registry.k8s.io/sig-storage/csi-provisioner:<none>
registry.k8s.io/sig-storage/csi-snapshotter:<none>
registry.k8s.io/sig-storage/snapshot-controller:<none>
registry.k8s.io/sig-storage/csi-external-health-monitor-controller:<none>
registry.k8s.io/sig-storage/csi-attacher:<none>
registry.k8s.io/sig-storage/csi-resizer:<none>
registry.k8s.io/sig-storage/hostpathplugin:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1230 19:39:30.030149 1664722 cache_images.go:85] Images are preloaded, skipping loading
I1230 19:39:30.030154 1664722 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1230 19:39:30.030218 1664722 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1230 19:39:30.030265 1664722 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1230 19:39:30.076168 1664722 cni.go:84] Creating CNI manager for ""
I1230 19:39:30.076193 1664722 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1230 19:39:30.076203 1664722 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1230 19:39:30.076219 1664722 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1230 19:39:30.076334 1664722 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1230 19:39:30.076428 1664722 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1230 19:39:30.083830 1664722 binaries.go:44] Found k8s binaries, skipping transfer
I1230 19:39:30.083915 1664722 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1230 19:39:30.091102 1664722 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1230 19:39:30.102135 1664722 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1230 19:39:30.115456 1664722 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1230 19:39:30.127537 1664722 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1230 19:39:30.130298 1664722 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1230 19:39:30.138370 1664722 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1230 19:39:30.181737 1664722 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1230 19:39:30.199677 1664722 certs.go:68] Setting up /home/aarav/.minikube/profiles/minikube for IP: 192.168.49.2
I1230 19:39:30.199683 1664722 certs.go:194] generating shared ca certs ...
I1230 19:39:30.199705 1664722 certs.go:226] acquiring lock for ca certs: {Name:mkb0b85a401da717dba871d5da5b1581d4ab59b5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1230 19:39:30.199799 1664722 certs.go:235] skipping valid "minikubeCA" ca cert: /home/aarav/.minikube/ca.key
I1230 19:39:30.199816 1664722 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/aarav/.minikube/proxy-client-ca.key
I1230 19:39:30.199819 1664722 certs.go:256] generating profile certs ...
I1230 19:39:30.199857 1664722 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/aarav/.minikube/profiles/minikube/client.key
I1230 19:39:30.207285 1664722 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/aarav/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1230 19:39:30.207344 1664722 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/aarav/.minikube/profiles/minikube/proxy-client.key
I1230 19:39:30.207549 1664722 certs.go:484] found cert: /home/aarav/.minikube/certs/ca-key.pem (1675 bytes)
I1230 19:39:30.207578 1664722 certs.go:484] found cert: /home/aarav/.minikube/certs/ca.pem (1074 bytes)
I1230 19:39:30.207599 1664722 certs.go:484] found cert: /home/aarav/.minikube/certs/cert.pem (1119 bytes)
I1230 19:39:30.207618 1664722 certs.go:484] found cert: /home/aarav/.minikube/certs/key.pem (1675 bytes)
I1230 19:39:30.208177 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1230 19:39:30.228459 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1230 19:39:30.244923 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1230 19:39:30.262087 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1230 19:39:30.291895 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1230 19:39:30.325322 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1230 19:39:30.346133 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1230 19:39:30.364301 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1230 19:39:30.385441 1664722 ssh_runner.go:362] scp /home/aarav/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1230 19:39:30.407789 1664722 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (742 bytes)
I1230 19:39:30.424156 1664722 ssh_runner.go:195] Run: openssl version
I1230 19:39:30.430619 1664722 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1230 19:39:30.439503 1664722 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1230 19:39:30.444277 1664722 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 23  2025 /usr/share/ca-certificates/minikubeCA.pem
I1230 19:39:30.444310 1664722 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1230 19:39:30.450492 1664722 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1230 19:39:30.458303 1664722 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1230 19:39:30.462790 1664722 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1230 19:39:30.469502 1664722 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1230 19:39:30.475202 1664722 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1230 19:39:30.480607 1664722 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1230 19:39:30.486901 1664722 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1230 19:39:30.492643 1664722 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1230 19:39:30.499542 1664722 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3688 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[csi-hostpath-driver:true default-storageclass:true storage-provisioner:true volumesnapshots:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1230 19:39:30.499655 1664722 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1230 19:39:30.515815 1664722 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1230 19:39:30.522869 1664722 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1230 19:39:30.522876 1664722 kubeadm.go:589] restartPrimaryControlPlane start ...
I1230 19:39:30.522936 1664722 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1230 19:39:30.529482 1664722 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1230 19:39:30.529559 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1230 19:39:30.565518 1664722 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/aarav/.kube/config
I1230 19:39:30.565590 1664722 kubeconfig.go:62] /home/aarav/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I1230 19:39:30.565848 1664722 lock.go:35] WriteFile acquiring /home/aarav/.kube/config: {Name:mk8309978973d7fc9c33dd82d425e947e97f5ea3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1230 19:39:30.567152 1664722 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1230 19:39:30.573771 1664722 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I1230 19:39:30.573789 1664722 kubeadm.go:593] duration metric: took 50.907954ms to restartPrimaryControlPlane
I1230 19:39:30.573796 1664722 kubeadm.go:394] duration metric: took 74.263096ms to StartCluster
I1230 19:39:30.573807 1664722 settings.go:142] acquiring lock: {Name:mk9fd45a977ebde25091b61f1a31b8a4b2899695 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1230 19:39:30.573860 1664722 settings.go:150] Updating kubeconfig:  /home/aarav/.kube/config
I1230 19:39:30.574271 1664722 lock.go:35] WriteFile acquiring /home/aarav/.kube/config: {Name:mk8309978973d7fc9c33dd82d425e947e97f5ea3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1230 19:39:30.574441 1664722 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1230 19:39:30.574494 1664722 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:true dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:true yakd:false]
I1230 19:39:30.574552 1664722 addons.go:69] Setting volumesnapshots=true in profile "minikube"
I1230 19:39:30.574570 1664722 addons.go:238] Setting addon volumesnapshots=true in "minikube"
I1230 19:39:30.574568 1664722 addons.go:69] Setting csi-hostpath-driver=true in profile "minikube"
W1230 19:39:30.574574 1664722 addons.go:247] addon volumesnapshots should already be in state true
I1230 19:39:30.574569 1664722 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1230 19:39:30.574581 1664722 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1230 19:39:30.574586 1664722 addons.go:238] Setting addon csi-hostpath-driver=true in "minikube"
I1230 19:39:30.574587 1664722 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1230 19:39:30.574590 1664722 addons.go:247] addon csi-hostpath-driver should already be in state true
I1230 19:39:30.574590 1664722 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1230 19:39:30.574592 1664722 host.go:66] Checking if "minikube" exists ...
W1230 19:39:30.574594 1664722 addons.go:247] addon storage-provisioner should already be in state true
I1230 19:39:30.574606 1664722 host.go:66] Checking if "minikube" exists ...
I1230 19:39:30.574616 1664722 host.go:66] Checking if "minikube" exists ...
I1230 19:39:30.574628 1664722 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1230 19:39:30.574821 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:30.574829 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:30.574852 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:30.574862 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:30.579956 1664722 out.go:179] üîé  Verifying Kubernetes components...
I1230 19:39:30.585763 1664722 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1230 19:39:30.651587 1664722 addons.go:238] Setting addon default-storageclass=true in "minikube"
W1230 19:39:30.651600 1664722 addons.go:247] addon default-storageclass should already be in state true
I1230 19:39:30.651615 1664722 host.go:66] Checking if "minikube" exists ...
I1230 19:39:30.651879 1664722 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1230 19:39:30.657043 1664722 out.go:179]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1230 19:39:30.662482 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/snapshot-controller:v6.1.0
I1230 19:39:30.662497 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/csi-attacher:v4.0.0
I1230 19:39:30.668160 1664722 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1230 19:39:30.668171 1664722 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1230 19:39:30.668254 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:30.673446 1664722 addons.go:435] installing /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml
I1230 19:39:30.673457 1664722 ssh_runner.go:362] scp volumesnapshots/csi-hostpath-snapshotclass.yaml --> /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml (934 bytes)
I1230 19:39:30.673566 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:30.678898 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/csi-external-health-monitor-controller:v0.7.0
I1230 19:39:30.684410 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/csi-node-driver-registrar:v2.6.0
I1230 19:39:30.690510 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/hostpathplugin:v1.9.0
I1230 19:39:30.696639 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/livenessprobe:v2.8.0
I1230 19:39:30.697250 1664722 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1230 19:39:30.701456 1664722 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1230 19:39:30.701468 1664722 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1230 19:39:30.701554 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:30.707933 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/csi-resizer:v1.6.0
I1230 19:39:30.714746 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/csi-snapshotter:v6.1.0
I1230 19:39:30.716047 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1230 19:39:30.717491 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:30.720982 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:30.723209 1664722 out.go:179]     ‚ñ™ Using image registry.k8s.io/sig-storage/csi-provisioner:v3.3.0
I1230 19:39:30.728814 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-external-attacher.yaml
I1230 19:39:30.728827 1664722 ssh_runner.go:362] scp csi-hostpath-driver/rbac/rbac-external-attacher.yaml --> /etc/kubernetes/addons/rbac-external-attacher.yaml (3073 bytes)
I1230 19:39:30.728889 1664722 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1230 19:39:30.759805 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:30.770271 1664722 api_server.go:52] waiting for apiserver process to appear ...
I1230 19:39:30.770345 1664722 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1230 19:39:30.780520 1664722 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:37117 SSHKeyPath:/home/aarav/.minikube/machines/minikube/id_rsa Username:docker}
I1230 19:39:30.824858 1664722 addons.go:435] installing /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
I1230 19:39:30.824873 1664722 ssh_runner.go:362] scp volumesnapshots/snapshot.storage.k8s.io_volumesnapshotclasses.yaml --> /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml (6471 bytes)
I1230 19:39:30.830277 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1230 19:39:30.842191 1664722 addons.go:435] installing /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
I1230 19:39:30.842202 1664722 ssh_runner.go:362] scp volumesnapshots/snapshot.storage.k8s.io_volumesnapshotcontents.yaml --> /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml (23126 bytes)
I1230 19:39:30.858422 1664722 addons.go:435] installing /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml
I1230 19:39:30.858433 1664722 ssh_runner.go:362] scp volumesnapshots/snapshot.storage.k8s.io_volumesnapshots.yaml --> /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml (19582 bytes)
I1230 19:39:30.869398 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1230 19:39:30.880126 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml
I1230 19:39:30.880139 1664722 ssh_runner.go:362] scp volumesnapshots/rbac-volume-snapshot-controller.yaml --> /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml (3545 bytes)
W1230 19:39:30.883050 1664722 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:30.883091 1664722 retry.go:31] will retry after 249.682345ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:30.884978 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-hostpath.yaml
I1230 19:39:30.884990 1664722 ssh_runner.go:362] scp csi-hostpath-driver/rbac/rbac-hostpath.yaml --> /etc/kubernetes/addons/rbac-hostpath.yaml (4266 bytes)
I1230 19:39:30.897199 1664722 addons.go:435] installing /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml
I1230 19:39:30.897209 1664722 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml (1475 bytes)
I1230 19:39:30.907415 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml
I1230 19:39:30.907426 1664722 ssh_runner.go:362] scp csi-hostpath-driver/rbac/rbac-external-health-monitor-controller.yaml --> /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml (3038 bytes)
I1230 19:39:30.916160 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml -f /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml -f /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml
W1230 19:39:30.919717 1664722 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:30.919736 1664722 retry.go:31] will retry after 138.760532ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:30.928606 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-external-provisioner.yaml
I1230 19:39:30.928621 1664722 ssh_runner.go:362] scp csi-hostpath-driver/rbac/rbac-external-provisioner.yaml --> /etc/kubernetes/addons/rbac-external-provisioner.yaml (4442 bytes)
I1230 19:39:30.951411 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-external-resizer.yaml
I1230 19:39:30.951424 1664722 ssh_runner.go:362] scp csi-hostpath-driver/rbac/rbac-external-resizer.yaml --> /etc/kubernetes/addons/rbac-external-resizer.yaml (2943 bytes)
I1230 19:39:30.967774 1664722 addons.go:435] installing /etc/kubernetes/addons/rbac-external-snapshotter.yaml
I1230 19:39:30.967790 1664722 ssh_runner.go:362] scp csi-hostpath-driver/rbac/rbac-external-snapshotter.yaml --> /etc/kubernetes/addons/rbac-external-snapshotter.yaml (3149 bytes)
W1230 19:39:30.980909 1664722 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml -f /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml -f /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:30.980926 1664722 retry.go:31] will retry after 283.551856ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml -f /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml -f /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:30.984603 1664722 addons.go:435] installing /etc/kubernetes/addons/csi-hostpath-attacher.yaml
I1230 19:39:30.984610 1664722 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/csi-hostpath-attacher.yaml (2143 bytes)
I1230 19:39:30.998439 1664722 addons.go:435] installing /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml
I1230 19:39:30.998449 1664722 ssh_runner.go:362] scp csi-hostpath-driver/deploy/csi-hostpath-driverinfo.yaml --> /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml (1274 bytes)
I1230 19:39:31.011897 1664722 addons.go:435] installing /etc/kubernetes/addons/csi-hostpath-plugin.yaml
I1230 19:39:31.011906 1664722 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/csi-hostpath-plugin.yaml (8201 bytes)
I1230 19:39:31.030378 1664722 addons.go:435] installing /etc/kubernetes/addons/csi-hostpath-resizer.yaml
I1230 19:39:31.030388 1664722 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/csi-hostpath-resizer.yaml (2191 bytes)
I1230 19:39:31.044602 1664722 addons.go:435] installing /etc/kubernetes/addons/csi-hostpath-storageclass.yaml
I1230 19:39:31.044617 1664722 ssh_runner.go:362] scp csi-hostpath-driver/deploy/csi-hostpath-storageclass.yaml --> /etc/kubernetes/addons/csi-hostpath-storageclass.yaml (846 bytes)
I1230 19:39:31.058758 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1230 19:39:31.058786 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/rbac-external-attacher.yaml -f /etc/kubernetes/addons/rbac-hostpath.yaml -f /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml -f /etc/kubernetes/addons/rbac-external-provisioner.yaml -f /etc/kubernetes/addons/rbac-external-resizer.yaml -f /etc/kubernetes/addons/rbac-external-snapshotter.yaml -f /etc/kubernetes/addons/csi-hostpath-attacher.yaml -f /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml -f /etc/kubernetes/addons/csi-hostpath-plugin.yaml -f /etc/kubernetes/addons/csi-hostpath-resizer.yaml -f /etc/kubernetes/addons/csi-hostpath-storageclass.yaml
W1230 19:39:31.103514 1664722 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/rbac-external-attacher.yaml -f /etc/kubernetes/addons/rbac-hostpath.yaml -f /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml -f /etc/kubernetes/addons/rbac-external-provisioner.yaml -f /etc/kubernetes/addons/rbac-external-resizer.yaml -f /etc/kubernetes/addons/rbac-external-snapshotter.yaml -f /etc/kubernetes/addons/csi-hostpath-attacher.yaml -f /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml -f /etc/kubernetes/addons/csi-hostpath-plugin.yaml -f /etc/kubernetes/addons/csi-hostpath-resizer.yaml -f /etc/kubernetes/addons/csi-hostpath-storageclass.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/rbac-external-attacher.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-hostpath.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-resizer.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-snapshotter.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-attacher.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-driverinfo.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-plugin.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-resizer.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1230 19:39:31.103530 1664722 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:31.103531 1664722 retry.go:31] will retry after 213.664968ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/rbac-external-attacher.yaml -f /etc/kubernetes/addons/rbac-hostpath.yaml -f /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml -f /etc/kubernetes/addons/rbac-external-provisioner.yaml -f /etc/kubernetes/addons/rbac-external-resizer.yaml -f /etc/kubernetes/addons/rbac-external-snapshotter.yaml -f /etc/kubernetes/addons/csi-hostpath-attacher.yaml -f /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml -f /etc/kubernetes/addons/csi-hostpath-plugin.yaml -f /etc/kubernetes/addons/csi-hostpath-resizer.yaml -f /etc/kubernetes/addons/csi-hostpath-storageclass.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/rbac-external-attacher.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-hostpath.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-resizer.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/rbac-external-snapshotter.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-attacher.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-driverinfo.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-plugin.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-resizer.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/csi-hostpath-storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:31.103540 1664722 retry.go:31] will retry after 547.635958ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:31.133501 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W1230 19:39:31.174537 1664722 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:31.174554 1664722 retry.go:31] will retry after 229.364624ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1230 19:39:31.264726 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml -f /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml -f /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml
I1230 19:39:31.270829 1664722 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1230 19:39:31.318085 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/rbac-external-attacher.yaml -f /etc/kubernetes/addons/rbac-hostpath.yaml -f /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml -f /etc/kubernetes/addons/rbac-external-provisioner.yaml -f /etc/kubernetes/addons/rbac-external-resizer.yaml -f /etc/kubernetes/addons/rbac-external-snapshotter.yaml -f /etc/kubernetes/addons/csi-hostpath-attacher.yaml -f /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml -f /etc/kubernetes/addons/csi-hostpath-plugin.yaml -f /etc/kubernetes/addons/csi-hostpath-resizer.yaml -f /etc/kubernetes/addons/csi-hostpath-storageclass.yaml
I1230 19:39:31.404599 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1230 19:39:31.651838 1664722 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1230 19:39:33.101519 1664722 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/csi-hostpath-snapshotclass.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotclasses.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshotcontents.yaml -f /etc/kubernetes/addons/snapshot.storage.k8s.io_volumesnapshots.yaml -f /etc/kubernetes/addons/rbac-volume-snapshot-controller.yaml -f /etc/kubernetes/addons/volume-snapshot-controller-deployment.yaml: (1.836749472s)
I1230 19:39:33.101560 1664722 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (1.830713575s)
I1230 19:39:33.101577 1664722 api_server.go:72] duration metric: took 2.527121078s to wait for apiserver process to appear ...
I1230 19:39:33.101582 1664722 api_server.go:88] waiting for apiserver healthz status ...
I1230 19:39:33.101595 1664722 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:43877/healthz ...
I1230 19:39:33.107009 1664722 api_server.go:279] https://127.0.0.1:43877/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1230 19:39:33.107034 1664722 api_server.go:103] status: https://127.0.0.1:43877/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1230 19:39:33.602541 1664722 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:43877/healthz ...
I1230 19:39:33.606509 1664722 api_server.go:279] https://127.0.0.1:43877/healthz returned 200:
ok
I1230 19:39:33.613230 1664722 api_server.go:141] control plane version: v1.34.0
I1230 19:39:33.613245 1664722 api_server.go:131] duration metric: took 511.658753ms to wait for apiserver health ...
I1230 19:39:33.613251 1664722 system_pods.go:43] waiting for kube-system pods to appear ...
I1230 19:39:33.616602 1664722 system_pods.go:59] 12 kube-system pods found
I1230 19:39:33.616615 1664722 system_pods.go:61] "coredns-66bc5c9577-8g5cd" [df9aabfb-9d45-480b-aaaa-8c9e826a13a2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1230 19:39:33.616621 1664722 system_pods.go:61] "csi-hostpath-attacher-0" [443dffe6-d217-4bd5-a9bf-e0dabedd3cb7] Running / Ready:ContainersNotReady (containers with unready status: [csi-attacher]) / ContainersReady:ContainersNotReady (containers with unready status: [csi-attacher])
I1230 19:39:33.616624 1664722 system_pods.go:61] "csi-hostpath-resizer-0" [b60f9724-73f7-4a21-9408-19f424593f8c] Running / Ready:ContainersNotReady (containers with unready status: [csi-resizer]) / ContainersReady:ContainersNotReady (containers with unready status: [csi-resizer])
I1230 19:39:33.616627 1664722 system_pods.go:61] "csi-hostpathplugin-5dkb6" [b9b8f723-5f09-4db9-8bab-055c6aa337e2] Running / Ready:ContainersNotReady (containers with unready status: [csi-external-health-monitor-controller node-driver-registrar hostpath liveness-probe csi-provisioner csi-snapshotter]) / ContainersReady:ContainersNotReady (containers with unready status: [csi-external-health-monitor-controller node-driver-registrar hostpath liveness-probe csi-provisioner csi-snapshotter])
I1230 19:39:33.616629 1664722 system_pods.go:61] "etcd-minikube" [ebe0a41f-1577-4c9f-a992-1fd9fda460b3] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1230 19:39:33.616631 1664722 system_pods.go:61] "kube-apiserver-minikube" [6fcd8eed-0915-4378-855e-51ebe52ca14b] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1230 19:39:33.616633 1664722 system_pods.go:61] "kube-controller-manager-minikube" [3e1d68b5-41bc-4423-8eca-3694507e75b9] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1230 19:39:33.616635 1664722 system_pods.go:61] "kube-proxy-4xgxd" [7e556772-6279-495a-b9f9-ac9ecddd13b7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1230 19:39:33.616637 1664722 system_pods.go:61] "kube-scheduler-minikube" [438b1b32-fa29-4562-8423-327bb34edde2] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1230 19:39:33.616639 1664722 system_pods.go:61] "snapshot-controller-7d9fbc56b8-dxplh" [783ad4cc-9082-4b17-a9e3-ca085d771c10] Running / Ready:ContainersNotReady (containers with unready status: [volume-snapshot-controller]) / ContainersReady:ContainersNotReady (containers with unready status: [volume-snapshot-controller])
I1230 19:39:33.616641 1664722 system_pods.go:61] "snapshot-controller-7d9fbc56b8-t6psm" [aaafcd0a-1a31-43b4-abeb-a06e7ee79907] Running / Ready:ContainersNotReady (containers with unready status: [volume-snapshot-controller]) / ContainersReady:ContainersNotReady (containers with unready status: [volume-snapshot-controller])
I1230 19:39:33.616643 1664722 system_pods.go:61] "storage-provisioner" [e21392fb-e8fd-4c01-8cb4-b51288b6a659] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1230 19:39:33.616646 1664722 system_pods.go:74] duration metric: took 3.392423ms to wait for pod list to return data ...
I1230 19:39:33.616652 1664722 kubeadm.go:578] duration metric: took 3.042197066s to wait for: map[apiserver:true system_pods:true]
I1230 19:39:33.616659 1664722 node_conditions.go:102] verifying NodePressure condition ...
I1230 19:39:33.620685 1664722 node_conditions.go:122] node storage ephemeral capacity is 65739308Ki
I1230 19:39:33.620700 1664722 node_conditions.go:123] node cpu capacity is 12
I1230 19:39:33.620718 1664722 node_conditions.go:105] duration metric: took 4.055547ms to run NodePressure ...
I1230 19:39:33.620726 1664722 start.go:241] waiting for startup goroutines ...
I1230 19:39:33.680507 1664722 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/rbac-external-attacher.yaml -f /etc/kubernetes/addons/rbac-hostpath.yaml -f /etc/kubernetes/addons/rbac-external-health-monitor-controller.yaml -f /etc/kubernetes/addons/rbac-external-provisioner.yaml -f /etc/kubernetes/addons/rbac-external-resizer.yaml -f /etc/kubernetes/addons/rbac-external-snapshotter.yaml -f /etc/kubernetes/addons/csi-hostpath-attacher.yaml -f /etc/kubernetes/addons/csi-hostpath-driverinfo.yaml -f /etc/kubernetes/addons/csi-hostpath-plugin.yaml -f /etc/kubernetes/addons/csi-hostpath-resizer.yaml -f /etc/kubernetes/addons/csi-hostpath-storageclass.yaml: (2.362395384s)
I1230 19:39:33.680525 1664722 addons.go:479] Verifying addon csi-hostpath-driver=true in "minikube"
I1230 19:39:33.680543 1664722 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.275930764s)
I1230 19:39:33.680572 1664722 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.028713165s)
I1230 19:39:33.688014 1664722 out.go:179] üîé  Verifying csi-hostpath-driver addon...
I1230 19:39:33.694205 1664722 kapi.go:75] Waiting for pod with label "kubernetes.io/minikube-addons=csi-hostpath-driver" in ns "kube-system" ...
I1230 19:39:33.697322 1664722 kapi.go:86] Found 3 Pods for label selector kubernetes.io/minikube-addons=csi-hostpath-driver
I1230 19:39:33.697334 1664722 kapi.go:107] duration metric: took 3.133363ms to wait for kubernetes.io/minikube-addons=csi-hostpath-driver ...
I1230 19:39:33.702806 1664722 out.go:179] üåü  Enabled addons: volumesnapshots, storage-provisioner, default-storageclass, csi-hostpath-driver
I1230 19:39:33.713517 1664722 addons.go:514] duration metric: took 3.139020414s for enable addons: enabled=[volumesnapshots storage-provisioner default-storageclass csi-hostpath-driver]
I1230 19:39:33.713547 1664722 start.go:246] waiting for cluster config update ...
I1230 19:39:33.713556 1664722 start.go:255] writing updated cluster config ...
I1230 19:39:33.713841 1664722 ssh_runner.go:195] Run: rm -f paused
I1230 19:39:33.751720 1664722 start.go:617] kubectl: 1.34.2, cluster: 1.34.0 (minor skew: 0)
I1230 19:39:33.757799 1664722 out.go:179] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 30 13:55:03 minikube dockerd[866]: time="2025-12-30T13:55:03.510218288Z" level=info msg="ignoring event" container=904cdc63b2f07e94b38fa1370314d4f641f08e87ace869f2d5a207d7d6a6a0c1 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:55:03 minikube dockerd[866]: time="2025-12-30T13:55:03.620341591Z" level=info msg="ignoring event" container=d0b55a6cc2d5bca0355a178c0da413274e4f9f11ce4ab542f8ada2b10ef7d500 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:55:03 minikube dockerd[866]: time="2025-12-30T13:55:03.621756340Z" level=info msg="ignoring event" container=ac86a2cfc1b4545eb42567fef15e16114cbe607fa8461a6b4ea0dd584e6ce317 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:55:03 minikube dockerd[866]: time="2025-12-30T13:55:03.635894853Z" level=info msg="ignoring event" container=27cb8fc5149d9015a2612cfc3a3d8b1f59c99ee5f799ab1dedb8cd45ac5de776 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:55:10 minikube cri-dockerd[1230]: time="2025-12-30T13:55:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [====>                                              ]  2.227MB/27.61MB"
Dec 30 13:55:20 minikube cri-dockerd[1230]: time="2025-12-30T13:55:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=======>                                           ]  4.176MB/27.61MB"
Dec 30 13:55:30 minikube cri-dockerd[1230]: time="2025-12-30T13:55:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [==========>                                        ]  5.848MB/27.61MB"
Dec 30 13:55:40 minikube cri-dockerd[1230]: time="2025-12-30T13:55:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [==============>                                    ]  7.797MB/27.61MB"
Dec 30 13:55:50 minikube cri-dockerd[1230]: time="2025-12-30T13:55:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=================>                                 ]  9.747MB/27.61MB"
Dec 30 13:56:00 minikube cri-dockerd[1230]: time="2025-12-30T13:56:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [====================>                              ]  11.42MB/27.61MB"
Dec 30 13:56:10 minikube cri-dockerd[1230]: time="2025-12-30T13:56:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=======================>                           ]  13.09MB/27.61MB"
Dec 30 13:56:20 minikube cri-dockerd[1230]: time="2025-12-30T13:56:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [===========================>                       ]  15.04MB/27.61MB"
Dec 30 13:56:30 minikube cri-dockerd[1230]: time="2025-12-30T13:56:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [==============================>                    ]  16.99MB/27.61MB"
Dec 30 13:56:40 minikube cri-dockerd[1230]: time="2025-12-30T13:56:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=================================>                 ]  18.66MB/27.61MB"
Dec 30 13:56:50 minikube cri-dockerd[1230]: time="2025-12-30T13:56:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=====================================>             ]  20.61MB/27.61MB"
Dec 30 13:57:00 minikube cri-dockerd[1230]: time="2025-12-30T13:57:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [=========================================>         ]  22.84MB/27.61MB"
Dec 30 13:57:10 minikube cri-dockerd[1230]: time="2025-12-30T13:57:10Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [============================================>      ]  24.51MB/27.61MB"
Dec 30 13:57:20 minikube cri-dockerd[1230]: time="2025-12-30T13:57:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: 6bff562a7abf: Downloading [===============================================>   ]  26.46MB/27.61MB"
Dec 30 13:57:25 minikube cri-dockerd[1230]: time="2025-12-30T13:57:25Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 30 13:57:25 minikube dockerd[866]: time="2025-12-30T13:57:25.943394212Z" level=info msg="ignoring event" container=4c2b096ce70f7299452f6957752bf9a89c3a989d974eba63fa8e53de4ce9a0af module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:57:27 minikube dockerd[866]: time="2025-12-30T13:57:27.126263062Z" level=info msg="ignoring event" container=da21a57fc8f12f1d452fac37b5c64fb5a1da4ab58d4791605e12c4cc647c981b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:57:32 minikube cri-dockerd[1230]: time="2025-12-30T13:57:32Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.2@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24"
Dec 30 13:57:32 minikube dockerd[866]: time="2025-12-30T13:57:32.687309623Z" level=info msg="ignoring event" container=e5a4a9d01822d12490ba12e098851a78b0fea3088e6fae901efe2a407d71f887 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:57:34 minikube dockerd[866]: time="2025-12-30T13:57:34.287590327Z" level=info msg="ignoring event" container=0ce4a389ac4cd94e6abae3470d4aa5272737e1f60163c6ae5425f7abc51e40bc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 30 13:57:39 minikube cri-dockerd[1230]: time="2025-12-30T13:57:39Z" level=info msg="Stop pulling image hashicorp/http-echo:latest: Status: Image is up to date for hashicorp/http-echo:latest"
Dec 30 13:57:47 minikube cri-dockerd[1230]: time="2025-12-30T13:57:47Z" level=info msg="Stop pulling image hashicorp/http-echo:latest: Status: Image is up to date for hashicorp/http-echo:latest"
Dec 30 13:57:53 minikube dockerd[866]: time="2025-12-30T13:57:53.840285805Z" level=warning msg="reference for unknown type: " digest="sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef" remote="registry.k8s.io/ingress-nginx/controller@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef"
Dec 30 13:58:12 minikube cri-dockerd[1230]: time="2025-12-30T13:58:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Waiting "
Dec 30 13:58:22 minikube cri-dockerd[1230]: time="2025-12-30T13:58:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9824c27679d3: Downloading [===========>                                       ]    888kB/3.8MB"
Dec 30 13:58:32 minikube cri-dockerd[1230]: time="2025-12-30T13:58:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9824c27679d3: Downloading [=======================>                           ]  1.776MB/3.8MB"
Dec 30 13:58:42 minikube cri-dockerd[1230]: time="2025-12-30T13:58:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9824c27679d3: Downloading [================================>                  ]  2.507MB/3.8MB"
Dec 30 13:58:52 minikube cri-dockerd[1230]: time="2025-12-30T13:58:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 9824c27679d3: Downloading [===============================================>   ]  3.604MB/3.8MB"
Dec 30 13:59:02 minikube cri-dockerd[1230]: time="2025-12-30T13:59:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [=>                                                 ]  1.043MB/33.68MB"
Dec 30 13:59:12 minikube cri-dockerd[1230]: time="2025-12-30T13:59:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [====>                                              ]  2.784MB/33.68MB"
Dec 30 13:59:22 minikube cri-dockerd[1230]: time="2025-12-30T13:59:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: c143cf4725f8: Downloading [=======>                                           ]  677.3kB/4.82MB"
Dec 30 13:59:32 minikube cri-dockerd[1230]: time="2025-12-30T13:59:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: c143cf4725f8: Downloading [======================>                            ]   2.14MB/4.82MB"
Dec 30 13:59:42 minikube cri-dockerd[1230]: time="2025-12-30T13:59:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [=======>                                           ]  5.221MB/33.68MB"
Dec 30 13:59:52 minikube cri-dockerd[1230]: time="2025-12-30T13:59:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [========>                                          ]  5.917MB/33.68MB"
Dec 30 14:00:02 minikube cri-dockerd[1230]: time="2025-12-30T14:00:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [==========>                                        ]   7.31MB/33.68MB"
Dec 30 14:00:12 minikube cri-dockerd[1230]: time="2025-12-30T14:00:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Downloading [=====>                                             ]  2.871MB/25.21MB"
Dec 30 14:00:22 minikube cri-dockerd[1230]: time="2025-12-30T14:00:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d29fa5b0e5e2: Downloading [=============================>                     ]  33.24kB/56.8kB"
Dec 30 14:00:32 minikube cri-dockerd[1230]: time="2025-12-30T14:00:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Downloading [============>                                      ]  6.265MB/25.21MB"
Dec 30 14:00:42 minikube cri-dockerd[1230]: time="2025-12-30T14:00:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Downloading [================>                                  ]  8.093MB/25.21MB"
Dec 30 14:00:52 minikube cri-dockerd[1230]: time="2025-12-30T14:00:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 5711bbb92cda: Downloading [===================>                               ]  9.921MB/25.21MB"
Dec 30 14:01:02 minikube cri-dockerd[1230]: time="2025-12-30T14:01:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [=============>                                     ]  9.399MB/33.68MB"
Dec 30 14:01:12 minikube cri-dockerd[1230]: time="2025-12-30T14:01:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: d44309a0b764: Downloading [=============================================>     ]  30.79MB/33.68MB"
Dec 30 14:01:22 minikube cri-dockerd[1230]: time="2025-12-30T14:01:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 13ee25061d00: Downloading [======>                                            ]  208.3kB/1.731MB"
Dec 30 14:01:32 minikube cri-dockerd[1230]: time="2025-12-30T14:01:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 13ee25061d00: Downloading [====================>                              ]  713.2kB/1.731MB"
Dec 30 14:01:42 minikube cri-dockerd[1230]: time="2025-12-30T14:01:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 13ee25061d00: Downloading [===================================>               ]  1.235MB/1.731MB"
Dec 30 14:01:52 minikube cri-dockerd[1230]: time="2025-12-30T14:01:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 13ee25061d00: Downloading [==========================================>        ]  1.462MB/1.731MB"
Dec 30 14:02:02 minikube cri-dockerd[1230]: time="2025-12-30T14:02:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 13ee25061d00: Downloading [===============================================>   ]  1.636MB/1.731MB"
Dec 30 14:02:12 minikube cri-dockerd[1230]: time="2025-12-30T14:02:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [====>                                              ]  1.809MB/22.11MB"
Dec 30 14:02:22 minikube cri-dockerd[1230]: time="2025-12-30T14:02:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [=======>                                           ]  3.393MB/22.11MB"
Dec 30 14:02:32 minikube cri-dockerd[1230]: time="2025-12-30T14:02:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [===========>                                       ]  5.203MB/22.11MB"
Dec 30 14:02:42 minikube cri-dockerd[1230]: time="2025-12-30T14:02:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [===============>                                   ]  6.788MB/22.11MB"
Dec 30 14:02:52 minikube cri-dockerd[1230]: time="2025-12-30T14:02:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [===================>                               ]  8.824MB/22.11MB"
Dec 30 14:03:02 minikube cri-dockerd[1230]: time="2025-12-30T14:03:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [========================>                          ]  10.86MB/22.11MB"
Dec 30 14:03:12 minikube cri-dockerd[1230]: time="2025-12-30T14:03:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [============================>                      ]  12.67MB/22.11MB"
Dec 30 14:03:22 minikube cri-dockerd[1230]: time="2025-12-30T14:03:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [================================>                  ]  14.48MB/22.11MB"
Dec 30 14:03:32 minikube cri-dockerd[1230]: time="2025-12-30T14:03:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.13.2@sha256:1f7eaeb01933e719c8a9f4acd8181e555e582330c7d50f24484fb64d2ba9b2ef: 28236ff56f2f: Downloading [====================================>              ]  16.07MB/22.11MB"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                                     ATTEMPT             POD ID              POD
9ea8a30f2ba85       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  5 minutes ago       Running             http-echo                                1                   e500d63ac4cb7       http-echo-deployment-d4f7b9959-4t4kq
d77e0a282ff0b       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  5 minutes ago       Running             http-echo                                1                   16f3bf66fcd72       http-echo-deployment-d4f7b9959-knbgt
e5a4a9d01822d       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24   6 minutes ago       Exited              patch                                    0                   0ce4a389ac4cd       ingress-nginx-admission-patch-mlcfz
4c2b096ce70f7       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:050a34002d5bb4966849c880c56c91f5320372564245733b33d4b3461b4dbd24   6 minutes ago       Exited              create                                   0                   da21a57fc8f12       ingress-nginx-admission-create-299kh
843e149c9e087       f9669ba5fa2b8                                                                                                                8 minutes ago       Running             node-disk-operator                       9                   791a2ad875dab       openebs-ndm-operator-f86d5ccd6-gnx9t
a4365cbffcf0b       5ab2c114fe2e9                                                                                                                8 minutes ago       Running             node-disk-exporter                       10                  07ce221d655f3       openebs-ndm-node-exporter-7zqb8
4c23736447543       5ab2c114fe2e9                                                                                                                8 minutes ago       Running             ndm-cluster-exporter                     9                   ba5a7becb00f5       openebs-ndm-cluster-exporter-556fc4c74d-c5fsk
42d4364f59229       382df876b172f                                                                                                                8 minutes ago       Running             openebs-provisioner-hostpath             9                   2325e8fa0f5fd       openebs-localpv-provisioner-5d77988647-5jrdt
037035361ccc5       6e38f40d628db                                                                                                                8 minutes ago       Running             storage-provisioner                      10                  7a4e3b4b10a91       storage-provisioner
bd4634ae87ba9       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  8 minutes ago       Running             http-echo                                1                   070f97871c555       http-echo-deployment-d4f7b9959-x858q
7e3f5b6d054f9       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  8 minutes ago       Running             http-echo                                1                   34cdab0c24d44       http-echo-deployment-d4f7b9959-ldz6p
65004e29a2a2d       738351fd438f0                                                                                                                9 minutes ago       Running             csi-snapshotter                          4                   168a857f9d4b0       csi-hostpathplugin-5dkb6
31916cee444cb       931dbfd16f87c                                                                                                                9 minutes ago       Running             csi-provisioner                          5                   168a857f9d4b0       csi-hostpathplugin-5dkb6
4c9615f24c33b       e899260153aed                                                                                                                9 minutes ago       Running             liveness-probe                           4                   168a857f9d4b0       csi-hostpathplugin-5dkb6
9e203ebc6fa6f       e255e073c508c                                                                                                                9 minutes ago       Running             hostpath                                 4                   168a857f9d4b0       csi-hostpathplugin-5dkb6
f4fdb51ac4290       88ef14a257f42                                                                                                                9 minutes ago       Running             node-driver-registrar                    4                   168a857f9d4b0       csi-hostpathplugin-5dkb6
da1a8a1a320f1       a1ed5895ba635                                                                                                                9 minutes ago       Running             csi-external-health-monitor-controller   5                   168a857f9d4b0       csi-hostpathplugin-5dkb6
842ba893b571f       59cbb42146a37                                                                                                                9 minutes ago       Running             csi-attacher                             5                   84a60b506f19f       csi-hostpath-attacher-0
365d23be6f58c       aa61ee9c70bc4                                                                                                                9 minutes ago       Running             volume-snapshot-controller               6                   a5ff98eb4be0c       snapshot-controller-7d9fbc56b8-dxplh
ac86a2cfc1b45       382df876b172f                                                                                                                9 minutes ago       Exited              openebs-provisioner-hostpath             8                   2325e8fa0f5fd       openebs-localpv-provisioner-5d77988647-5jrdt
27cb8fc5149d9       5ab2c114fe2e9                                                                                                                9 minutes ago       Exited              node-disk-exporter                       9                   07ce221d655f3       openebs-ndm-node-exporter-7zqb8
bde66196b7e13       19a639eda60f0                                                                                                                9 minutes ago       Running             csi-resizer                              5                   fb077b6175307       csi-hostpath-resizer-0
d0b55a6cc2d5b       f9669ba5fa2b8                                                                                                                9 minutes ago       Exited              node-disk-operator                       8                   791a2ad875dab       openebs-ndm-operator-f86d5ccd6-gnx9t
904cdc63b2f07       5ab2c114fe2e9                                                                                                                9 minutes ago       Exited              ndm-cluster-exporter                     8                   ba5a7becb00f5       openebs-ndm-cluster-exporter-556fc4c74d-c5fsk
c60b8f6bcd779       52546a367cc9e                                                                                                                9 minutes ago       Running             coredns                                  5                   7269fa5ea9113       coredns-66bc5c9577-8g5cd
19b4d1b02c962       aa61ee9c70bc4                                                                                                                9 minutes ago       Running             volume-snapshot-controller               5                   9e83e14f57075       snapshot-controller-7d9fbc56b8-t6psm
55b6deb657166       6e38f40d628db                                                                                                                9 minutes ago       Exited              storage-provisioner                      9                   7a4e3b4b10a91       storage-provisioner
d94e3001d887e       df0860106674d                                                                                                                9 minutes ago       Running             kube-proxy                               5                   845323466f9a7       kube-proxy-4xgxd
f1d845665838c       46169d968e920                                                                                                                9 minutes ago       Running             kube-scheduler                           5                   10b7e7537dc5d       kube-scheduler-minikube
8afc32e38a728       90550c43ad2bc                                                                                                                9 minutes ago       Running             kube-apiserver                           5                   85456a6daabd8       kube-apiserver-minikube
ddb76f53165fe       a0af72f2ec6d6                                                                                                                9 minutes ago       Running             kube-controller-manager                  5                   fb71fbca3fd76       kube-controller-manager-minikube
122fb2c51bbae       5f1f5298c888d                                                                                                                9 minutes ago       Running             etcd                                     5                   eaa161e8bb024       etcd-minikube
34293ba7f83e8       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  18 minutes ago      Exited              http-echo                                0                   17ce6421b2baf       http-echo-deployment-d4f7b9959-x858q
e6b956f7b3a62       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  19 minutes ago      Exited              http-echo                                0                   2ebf55f80afda       http-echo-deployment-d4f7b9959-ldz6p
23370cf2f0a68       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  19 minutes ago      Exited              http-echo                                0                   3f527279b6953       http-echo-deployment-d4f7b9959-knbgt
68be357eec34f       hashicorp/http-echo@sha256:fcb75f691c8b0414d670ae570240cbf95502cc18a9ba57e982ecac589760a186                                  19 minutes ago      Exited              http-echo                                0                   7c302bc02ad2b       http-echo-deployment-d4f7b9959-4t4kq
c5ab459e49cdd       738351fd438f0                                                                                                                27 minutes ago      Exited              csi-snapshotter                          3                   98548e026d573       csi-hostpathplugin-5dkb6
5c0c7d21935bb       931dbfd16f87c                                                                                                                27 minutes ago      Exited              csi-provisioner                          4                   98548e026d573       csi-hostpathplugin-5dkb6
95c8a04000995       e899260153aed                                                                                                                27 minutes ago      Exited              liveness-probe                           3                   98548e026d573       csi-hostpathplugin-5dkb6
43675d444d5fa       e255e073c508c                                                                                                                27 minutes ago      Exited              hostpath                                 3                   98548e026d573       csi-hostpathplugin-5dkb6
0cdffc9d91d06       52546a367cc9e                                                                                                                27 minutes ago      Exited              coredns                                  4                   eaff73f9b5873       coredns-66bc5c9577-8g5cd
3c3ba821312fe       46169d968e920                                                                                                                27 minutes ago      Exited              kube-scheduler                           4                   27e8017fa619b       kube-scheduler-minikube
ff6340458a04c       19a639eda60f0                                                                                                                27 minutes ago      Exited              csi-resizer                              4                   445ae6b40a348       csi-hostpath-resizer-0
aa113a4051a76       df0860106674d                                                                                                                27 minutes ago      Exited              kube-proxy                               4                   b94d12d5e7ad2       kube-proxy-4xgxd
f58cf6a869f78       5f1f5298c888d                                                                                                                27 minutes ago      Exited              etcd                                     4                   ae55243548b90       etcd-minikube
e7eb95473d880       59cbb42146a37                                                                                                                27 minutes ago      Exited              csi-attacher                             4                   2f0452ec61d16       csi-hostpath-attacher-0
b63cbf8259f31       88ef14a257f42                                                                                                                27 minutes ago      Exited              node-driver-registrar                    3                   98548e026d573       csi-hostpathplugin-5dkb6
38f853585dfe4       a0af72f2ec6d6                                                                                                                27 minutes ago      Exited              kube-controller-manager                  4                   a6cf65501b330       kube-controller-manager-minikube
c0be8aeed5869       aa61ee9c70bc4                                                                                                                27 minutes ago      Exited              volume-snapshot-controller               4                   7d68c82647a50       snapshot-controller-7d9fbc56b8-t6psm
8b0d972f3d969       aa61ee9c70bc4                                                                                                                27 minutes ago      Exited              volume-snapshot-controller               5                   c11f5bc812d41       snapshot-controller-7d9fbc56b8-dxplh
38f457f743d90       90550c43ad2bc                                                                                                                27 minutes ago      Exited              kube-apiserver                           4                   b9994fd4768c8       kube-apiserver-minikube
a041b69cabcb8       a1ed5895ba635                                                                                                                27 minutes ago      Exited              csi-external-health-monitor-controller   4                   98548e026d573       csi-hostpathplugin-5dkb6
2d13a0a6428fd       297e6f2690c43                                                                                                                8 days ago          Exited              node-disk-manager                        0                   e5c2bb9898ca8       openebs-ndm-hjwj8


==> coredns [0cdffc9d91d0] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: services is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "services" in API group "" at the cluster scope
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: namespaces is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "namespaces" in API group "" at the cluster scope
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: endpointslices.discovery.k8s.io is forbidden: User "system:serviceaccount:kube-system:coredns" cannot list resource "endpointslices" in API group "discovery.k8s.io" at the cluster scope
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:55976 - 14353 "HINFO IN 4765915778455580470.1411694204494285. udp 54 false 512" NXDOMAIN qr,rd,ra 54 0.003209237s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [c60b8f6bcd77] <==
maxprocs: Leaving GOMAXPROCS=12: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] 127.0.0.1:41725 - 11032 "HINFO IN 3029262309318808707.6571565058019327368. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.004440952s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] 10.244.0.105:41341 - 1799 "AAAA IN www.google-analytics.com.openebs.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000230004s
[INFO] 10.244.0.105:43321 - 60332 "A IN www.google-analytics.com.openebs.svc.cluster.local. udp 68 false 512" NXDOMAIN qr,aa,rd 161 0.000237069s
[INFO] 10.244.0.105:57768 - 28856 "A IN www.google-analytics.com.svc.cluster.local. udp 60 false 512" NXDOMAIN qr,aa,rd 153 0.000051944s
[INFO] 10.244.0.105:37703 - 13137 "AAAA IN www.google-analytics.com.svc.cluster.local. udp 60 false 512" NXDOMAIN qr,aa,rd 153 0.000039764s
[INFO] 10.244.0.105:35174 - 26191 "A IN www.google-analytics.com.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.000033967s
[INFO] 10.244.0.105:50987 - 38895 "AAAA IN www.google-analytics.com.cluster.local. udp 56 false 512" NXDOMAIN qr,aa,rd 149 0.00002914s
[INFO] 10.244.0.105:48998 - 65530 "AAAA IN www.google-analytics.com. udp 42 false 512" NOERROR qr,rd,ra 562 0.891608469s
[INFO] 10.244.0.105:34819 - 39018 "A IN www.google-analytics.com. udp 42 false 512" NOERROR qr,rd,ra 282 0.891879278s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_12_22T12_21_03_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
                    topology.hostpath.csi/node=minikube
Annotations:        csi.volume.kubernetes.io/nodeid: {"hostpath.csi.k8s.io":"minikube"}
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 22 Dec 2025 06:35:59 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 30 Dec 2025 14:03:34 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 30 Dec 2025 14:00:39 +0000   Tue, 30 Dec 2025 13:27:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 30 Dec 2025 14:00:39 +0000   Tue, 30 Dec 2025 13:27:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 30 Dec 2025 14:00:39 +0000   Tue, 30 Dec 2025 13:27:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 30 Dec 2025 14:00:39 +0000   Tue, 30 Dec 2025 13:27:34 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  65739308Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3825816Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  65739308Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3825816Ki
  pods:               110
System Info:
  Machine ID:                 1db52722f56349cfbe62334a71256d6b
  System UUID:                1db52722f56349cfbe62334a71256d6b
  Boot ID:                    27348c34-a606-4bb6-94c4-bc9999f5ffae
  Kernel Version:             6.6.26-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (22 in total)
  Namespace                   Name                                             CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                             ------------  ----------  ---------------  -------------  ---
  default                     http-echo-deployment-d4f7b9959-4t4kq             0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m
  default                     http-echo-deployment-d4f7b9959-knbgt             0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m
  default                     http-echo-deployment-d4f7b9959-ldz6p             0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m
  default                     http-echo-deployment-d4f7b9959-x858q             0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m
  ingress-nginx               ingress-nginx-controller-9cc49f96f-77n74         100m (0%)     0 (0%)      90Mi (2%)        0 (0%)         13m
  kube-system                 coredns-66bc5c9577-8g5cd                         100m (0%)     0 (0%)      70Mi (1%)        170Mi (4%)     8d
  kube-system                 csi-hostpath-attacher-0                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 csi-hostpath-resizer-0                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 csi-hostpathplugin-5dkb6                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 etcd-minikube                                    100m (0%)     0 (0%)      100Mi (2%)       0 (0%)         8d
  kube-system                 kube-apiserver-minikube                          250m (2%)     0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 kube-controller-manager-minikube                 200m (1%)     0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 kube-proxy-4xgxd                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 kube-scheduler-minikube                          100m (0%)     0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 snapshot-controller-7d9fbc56b8-dxplh             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 snapshot-controller-7d9fbc56b8-t6psm             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  kube-system                 storage-provisioner                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  openebs                     openebs-localpv-provisioner-5d77988647-5jrdt     0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  openebs                     openebs-ndm-cluster-exporter-556fc4c74d-c5fsk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  openebs                     openebs-ndm-hjwj8                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  openebs                     openebs-ndm-node-exporter-7zqb8                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
  openebs                     openebs-ndm-operator-f86d5ccd6-gnx9t             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (7%)   0 (0%)
  memory             260Mi (6%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                  From             Message
  ----     ------                   ----                 ----             -------
  Normal   Starting                 9m1s                 kube-proxy       
  Normal   Starting                 27m                  kube-proxy       
  Normal   NodeNotReady             36m                  node-controller  Node minikube status is now: NodeNotReady
  Normal   NodeNotReady             36m                  kubelet          Node minikube status is now: NodeNotReady
  Normal   NodeHasSufficientMemory  36m (x24 over 23h)   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    36m (x24 over 23h)   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     36m (x24 over 23h)   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeReady                36m                  kubelet          Node minikube status is now: NodeReady
  Warning  ContainerGCFailed        27m                  kubelet          rpc error: code = Unavailable desc = connection error: desc = "error reading server preface: read unix @->/run/cri-dockerd.sock: read: connection reset by peer"
  Normal   RegisteredNode           26m                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 9m5s                 kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  9m5s (x8 over 9m5s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    9m5s (x8 over 9m5s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     9m5s (x7 over 9m5s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  9m5s                 kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           9m                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec29 04:32] Spectre V2 : WARNING: Unprivileged eBPF is enabled with eIBRS on, data leaks possible via Spectre v2 BHB attacks!
[  +0.147803] Hangcheck: starting hangcheck timer 0.9.1 (tick is 180 seconds, margin is 60 seconds).
[  +0.142716] netlink: 'init': attribute type 4 has an invalid length.
[  +0.032349] fakeowner: loading out-of-tree module taints kernel.
[Dec29 10:05] Hangcheck: hangcheck value past margin!
[Dec29 11:13] Hangcheck: hangcheck value past margin!
[Dec29 13:28] Hangcheck: hangcheck value past margin!
[Dec29 14:05] systemd[5358]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[Dec30 13:27] Hangcheck: hangcheck value past margin!
[  +1.882025] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 1095475689 wd_nsec: 1095475631
[Dec30 13:49] Hangcheck: hangcheck value past margin!


==> etcd [122fb2c51bba] <==
{"level":"warn","ts":"2025-12-30T13:54:31.664773Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57252","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.669974Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57262","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.676544Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57290","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.681913Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57298","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.688885Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57316","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.694729Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57350","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.698741Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57354","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.702932Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57368","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.708881Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57386","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.715885Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57414","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.720566Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57428","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.724922Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57450","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.744265Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57472","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.750545Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57488","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.758532Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57516","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.767663Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57530","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.775838Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57540","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.780112Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57558","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.787244Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57586","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.794507Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.800643Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57622","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.807016Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57650","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.823604Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57670","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.830725Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57698","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.835012Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57710","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.838569Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57732","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.842272Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57758","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.846400Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57776","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.854216Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57782","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.865586Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57794","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.872466Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57800","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.877564Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57818","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.882260Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57846","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.885267Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57856","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.888509Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57874","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.895622Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57900","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.912640Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57922","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.917112Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57942","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.920801Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57964","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.925767Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57970","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.931516Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57978","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.935946Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:57992","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.940370Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58002","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.943904Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58018","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.949916Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58028","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.956096Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58038","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.964890Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58054","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.969681Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58066","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.988896Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58080","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.992960Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58090","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:31.997488Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58106","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:32.029735Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58124","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:32.397329Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58148","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:32.401518Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:58168","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:34.389601Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:34.392999Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52670","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:35.585357Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52688","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:35.597940Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52714","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:35.604484Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52730","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:54:35.609513Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:52756","server-name":"","error":"EOF"}


==> etcd [f58cf6a869f7] <==
{"level":"warn","ts":"2025-12-30T13:36:32.587996Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50112","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.599444Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50122","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.605764Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50142","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.610229Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50158","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.623384Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50182","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.630104Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50202","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.637723Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50214","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.643269Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50238","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.649906Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50260","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.655349Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50276","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.660093Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50294","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.666003Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50322","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.679726Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50348","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.684164Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50380","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.689879Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50394","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.696038Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50412","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.700436Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50430","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.704824Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50446","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.708987Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50458","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.715565Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50490","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.720760Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50502","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.726880Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50530","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.731007Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50550","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.735776Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50564","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.752810Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50592","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.756307Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50600","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.759726Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50608","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:32.787622Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50630","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:33.013443Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50646","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:33.028247Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50662","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:33.051270Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50690","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:33.057259Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50710","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:33.162958Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50724","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:33.168948Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:50752","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:36.323924Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56448","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-12-30T13:36:36.339299Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:56470","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-12-30T13:46:32.500174Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":82548}
{"level":"info","ts":"2025-12-30T13:46:32.559041Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":82548,"took":"57.763982ms","hash":3065223751,"current-db-size-bytes":5500928,"current-db-size":"5.5 MB","current-db-size-in-use-bytes":2596864,"current-db-size-in-use":"2.6 MB"}
{"level":"info","ts":"2025-12-30T13:46:32.559100Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3065223751,"revision":82548,"compact-revision":80664}
{"level":"info","ts":"2025-12-30T13:51:32.524541Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":83194}
{"level":"info","ts":"2025-12-30T13:51:32.578060Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":83194,"took":"52.739774ms","hash":2453675402,"current-db-size-bytes":5500928,"current-db-size":"5.5 MB","current-db-size-in-use-bytes":2760704,"current-db-size-in-use":"2.8 MB"}
{"level":"info","ts":"2025-12-30T13:51:32.578110Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2453675402,"revision":83194,"compact-revision":82548}
{"level":"info","ts":"2025-12-30T13:53:29.665260Z","caller":"osutil/interrupt_unix.go:65","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-12-30T13:53:29.665353Z","caller":"embed/etcd.go:426","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"error","ts":"2025-12-30T13:53:29.665452Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-12-30T13:53:36.670597Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"http: Server closed","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*serveCtx).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/serve.go:90"}
{"level":"error","ts":"2025-12-30T13:53:36.670660Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2381: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-12-30T13:53:36.670679Z","caller":"etcdserver/server.go:1281","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-12-30T13:53:36.670765Z","caller":"etcdserver/server.go:2342","msg":"server has stopped; stopping storage version's monitor"}
{"level":"info","ts":"2025-12-30T13:53:36.670776Z","caller":"etcdserver/server.go:2319","msg":"server has stopped; stopping cluster version's monitor"}
{"level":"warn","ts":"2025-12-30T13:53:36.670875Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-12-30T13:53:36.670911Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-12-30T13:53:36.670894Z","caller":"embed/serve.go:245","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"error","ts":"2025-12-30T13:53:36.670921Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"warn","ts":"2025-12-30T13:53:36.670926Z","caller":"embed/serve.go:247","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"error","ts":"2025-12-30T13:53:36.670936Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 127.0.0.1:2379: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-12-30T13:53:36.688207Z","caller":"embed/etcd.go:621","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"error","ts":"2025-12-30T13:53:36.688420Z","caller":"embed/etcd.go:912","msg":"setting up serving from embedded etcd failed.","error":"accept tcp 192.168.49.2:2380: use of closed network connection","stacktrace":"go.etcd.io/etcd/server/v3/embed.(*Etcd).errHandler\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:912\ngo.etcd.io/etcd/server/v3/embed.(*Etcd).startHandler.func1\n\tgo.etcd.io/etcd/server/v3/embed/etcd.go:906"}
{"level":"info","ts":"2025-12-30T13:53:36.688504Z","caller":"embed/etcd.go:626","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-12-30T13:53:36.688531Z","caller":"embed/etcd.go:428","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 14:03:35 up 1 day,  9:31,  0 users,  load average: 2.91, 3.72, 8.44
Linux minikube 6.6.26-linuxkit #1 SMP PREEMPT_DYNAMIC Sat Apr 27 04:15:35 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [38f457f743d9] <==
W1230 13:53:38.056330       1 logging.go:55] [core] [Channel #91 SubChannel #93]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.087267       1 logging.go:55] [core] [Channel #199 SubChannel #201]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.136817       1 logging.go:55] [core] [Channel #191 SubChannel #193]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.179307       1 logging.go:55] [core] [Channel #35 SubChannel #37]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.182414       1 logging.go:55] [core] [Channel #139 SubChannel #141]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.190774       1 logging.go:55] [core] [Channel #263 SubChannel #265]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.193931       1 logging.go:55] [core] [Channel #95 SubChannel #97]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.205004       1 logging.go:55] [core] [Channel #127 SubChannel #129]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.211853       1 logging.go:55] [core] [Channel #255 SubChannel #257]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.214886       1 logging.go:55] [core] [Channel #231 SubChannel #233]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.285276       1 logging.go:55] [core] [Channel #247 SubChannel #249]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.404761       1 logging.go:55] [core] [Channel #179 SubChannel #181]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.410298       1 logging.go:55] [core] [Channel #195 SubChannel #197]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.423855       1 logging.go:55] [core] [Channel #43 SubChannel #45]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.443281       1 logging.go:55] [core] [Channel #31 SubChannel #33]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.487194       1 logging.go:55] [core] [Channel #63 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.491644       1 logging.go:55] [core] [Channel #279 SubChannel #281]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.522390       1 logging.go:55] [core] [Channel #203 SubChannel #205]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.537417       1 logging.go:55] [core] [Channel #71 SubChannel #73]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.572715       1 logging.go:55] [core] [Channel #131 SubChannel #133]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.589438       1 logging.go:55] [core] [Channel #275 SubChannel #277]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.621825       1 logging.go:55] [core] [Channel #235 SubChannel #237]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.626785       1 logging.go:55] [core] [Channel #259 SubChannel #261]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.640900       1 logging.go:55] [core] [Channel #239 SubChannel #241]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.691835       1 logging.go:55] [core] [Channel #39 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.722895       1 logging.go:55] [core] [Channel #287 SubChannel #289]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.745264       1 logging.go:55] [core] [Channel #111 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.757344       1 logging.go:55] [core] [Channel #115 SubChannel #117]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.766175       1 logging.go:55] [core] [Channel #151 SubChannel #153]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.772895       1 logging.go:55] [core] [Channel #135 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.805918       1 logging.go:55] [core] [Channel #243 SubChannel #245]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.848485       1 logging.go:55] [core] [Channel #163 SubChannel #165]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.942465       1 logging.go:55] [core] [Channel #175 SubChannel #177]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.948287       1 logging.go:55] [core] [Channel #215 SubChannel #217]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:38.985341       1 logging.go:55] [core] [Channel #7 SubChannel #9]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.003901       1 logging.go:55] [core] [Channel #267 SubChannel #269]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.005471       1 logging.go:55] [core] [Channel #271 SubChannel #273]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.026932       1 logging.go:55] [core] [Channel #59 SubChannel #61]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.060602       1 logging.go:55] [core] [Channel #147 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.108732       1 logging.go:55] [core] [Channel #1 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.196379       1 logging.go:55] [core] [Channel #107 SubChannel #109]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.268070       1 logging.go:55] [core] [Channel #67 SubChannel #69]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.277992       1 logging.go:55] [core] [Channel #119 SubChannel #121]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.313475       1 logging.go:55] [core] [Channel #4 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.343765       1 logging.go:55] [core] [Channel #75 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.350321       1 logging.go:55] [core] [Channel #227 SubChannel #229]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.371869       1 logging.go:55] [core] [Channel #79 SubChannel #81]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.387404       1 logging.go:55] [core] [Channel #103 SubChannel #105]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.416678       1 logging.go:55] [core] [Channel #283 SubChannel #285]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.431318       1 logging.go:55] [core] [Channel #155 SubChannel #157]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.436806       1 logging.go:55] [core] [Channel #83 SubChannel #85]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.469429       1 logging.go:55] [core] [Channel #159 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.479189       1 logging.go:55] [core] [Channel #211 SubChannel #213]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.518453       1 logging.go:55] [core] [Channel #123 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.523183       1 logging.go:55] [core] [Channel #219 SubChannel #221]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.568717       1 logging.go:55] [core] [Channel #55 SubChannel #57]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.579738       1 logging.go:55] [core] [Channel #47 SubChannel #49]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.631247       1 logging.go:55] [core] [Channel #187 SubChannel #189]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.648917       1 logging.go:55] [core] [Channel #21 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W1230 13:53:39.689798       1 logging.go:55] [core] [Channel #171 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [8afc32e38a72] <==
I1230 13:54:32.235252       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1230 13:54:32.235259       1 shared_informer.go:349] "Waiting for caches to sync" controller="crd-autoregister"
I1230 13:54:32.237216       1 remote_available_controller.go:425] Starting RemoteAvailability controller
I1230 13:54:32.237239       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1230 13:54:32.237476       1 controller.go:119] Starting legacy_token_tracking_controller
I1230 13:54:32.237523       1 shared_informer.go:349] "Waiting for caches to sync" controller="configmaps"
I1230 13:54:32.237583       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1230 13:54:32.329501       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1230 13:54:32.334490       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1230 13:54:32.334516       1 policy_source.go:240] refreshing policies
I1230 13:54:32.334617       1 cache.go:39] Caches are synced for LocalAvailability controller
I1230 13:54:32.334630       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1230 13:54:32.334770       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1230 13:54:32.335528       1 shared_informer.go:356] "Caches are synced" controller="kubernetes-service-cidr-controller"
I1230 13:54:32.335552       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1230 13:54:32.335562       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1230 13:54:32.335572       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1230 13:54:32.335650       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1230 13:54:32.335768       1 handler.go:285] Adding GroupVersion snapshot.storage.k8s.io v1 to ResourceManager
I1230 13:54:32.335788       1 handler.go:285] Adding GroupVersion snapshot.storage.k8s.io v1beta1 to ResourceManager
I1230 13:54:32.335800       1 handler.go:285] Adding GroupVersion openebs.io v1alpha1 to ResourceManager
I1230 13:54:32.335900       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1230 13:54:32.336459       1 shared_informer.go:356] "Caches are synced" controller="crd-autoregister"
I1230 13:54:32.336518       1 aggregator.go:171] initial CRD sync complete...
I1230 13:54:32.336527       1 autoregister_controller.go:144] Starting autoregister controller
I1230 13:54:32.336532       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1230 13:54:32.336537       1 cache.go:39] Caches are synced for autoregister controller
I1230 13:54:32.337479       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1230 13:54:32.338469       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1230 13:54:32.338650       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1230 13:54:32.350765       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
W1230 13:54:32.397340       1 logging.go:55] [core] [Channel #259 SubChannel #260]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1230 13:54:32.401516       1 logging.go:55] [core] [Channel #263 SubChannel #264]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
I1230 13:54:32.462891       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I1230 13:54:33.241503       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W1230 13:54:34.389576       1 logging.go:55] [core] [Channel #267 SubChannel #268]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1230 13:54:34.392996       1 logging.go:55] [core] [Channel #271 SubChannel #272]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1230 13:54:35.582091       1 logging.go:55] [core] [Channel #275 SubChannel #276]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: authentication handshake failed: context canceled"
W1230 13:54:35.597896       1 logging.go:55] [core] [Channel #279 SubChannel #280]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1230 13:54:35.604248       1 logging.go:55] [core] [Channel #283 SubChannel #284]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
W1230 13:54:35.609506       1 logging.go:55] [core] [Channel #287 SubChannel #288]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", BalancerAttributes: {"<%!p(pickfirstleaf.managedByPickfirstKeyType={})>": "<%!p(bool=true)>" }}. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
I1230 13:54:35.652961       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1230 13:54:36.061941       1 controller.go:667] quota admission added evaluator for: endpoints
I1230 13:54:36.154404       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1230 13:54:36.262898       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1230 13:55:03.950451       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1230 13:55:57.182571       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 13:56:00.164772       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 13:57:07.858987       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 13:57:26.334363       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 13:57:28.250204       1 controller.go:667] quota admission added evaluator for: jobs.batch
I1230 13:58:27.424069       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 13:58:48.804556       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 13:59:43.646841       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 14:00:05.174827       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 14:01:07.509093       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 14:01:10.931987       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 14:02:08.033717       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 14:02:32.083820       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1230 14:03:16.897339       1 stats.go:136] "Error getting keys" err="empty key: \"\""


==> kube-controller-manager [38f853585dfe] <==
I1230 13:36:36.258875       1 disruption.go:468] "Starting disruption controller" logger="disruption-controller"
I1230 13:36:36.258907       1 shared_informer.go:349] "Waiting for caches to sync" controller="disruption"
I1230 13:36:36.304854       1 controllermanager.go:781] "Started controller" controller="cronjob-controller"
I1230 13:36:36.306868       1 cronjob_controllerv2.go:145] "Starting cronjob controller v2" logger="cronjob-controller"
I1230 13:36:36.306888       1 shared_informer.go:349] "Waiting for caches to sync" controller="cronjob"
I1230 13:36:36.308522       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1230 13:36:36.328866       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1230 13:36:36.332928       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1230 13:36:36.357764       1 shared_informer.go:356] "Caches are synced" controller="node"
I1230 13:36:36.357782       1 shared_informer.go:356] "Caches are synced" controller="job"
I1230 13:36:36.357790       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1230 13:36:36.357820       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1230 13:36:36.357843       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1230 13:36:36.357851       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1230 13:36:36.357855       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1230 13:36:36.359666       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1230 13:36:36.359717       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1230 13:36:36.360968       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1230 13:36:36.360977       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1230 13:36:36.363747       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1230 13:36:36.364744       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1230 13:36:36.367742       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1230 13:36:36.369732       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1230 13:36:36.370547       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1230 13:36:36.373732       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1230 13:36:36.373750       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1230 13:36:36.373732       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1230 13:36:36.373751       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1230 13:36:36.375738       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1230 13:36:36.377716       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1230 13:36:36.379720       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1230 13:36:36.379728       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1230 13:36:36.381708       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1230 13:36:36.383722       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1230 13:36:36.383722       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1230 13:36:36.385711       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1230 13:36:36.388721       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1230 13:36:36.405769       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1230 13:36:36.405786       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1230 13:36:36.405795       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1230 13:36:36.407741       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1230 13:36:36.408635       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1230 13:36:36.408870       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1230 13:36:36.409483       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1230 13:36:36.416797       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1230 13:36:36.422745       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1230 13:36:36.422843       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1230 13:36:36.422951       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1230 13:36:36.423003       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1230 13:36:36.423061       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1230 13:36:36.425782       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1230 13:36:36.428428       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1230 13:36:36.431738       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1230 13:36:36.434729       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1230 13:36:37.368220       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1230 13:36:37.368834       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1230 13:36:37.368898       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1230 13:36:37.368919       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1230 13:36:37.408890       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1230 13:36:37.431013       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [ddb76f53165f] <==
I1230 13:54:35.552742       1 controllermanager.go:733] "Controller is disabled by a feature gate" controller="kube-apiserver-serving-clustertrustbundle-publisher-controller" requiredFeatureGates=["ClusterTrustBundle"]
I1230 13:54:35.552763       1 controllermanager.go:733] "Controller is disabled by a feature gate" controller="device-taint-eviction-controller" requiredFeatureGates=["DynamicResourceAllocation","DRADeviceTaints"]
I1230 13:54:35.554470       1 disruption.go:457] "Sending events to api server." logger="disruption-controller"
I1230 13:54:35.554543       1 disruption.go:468] "Starting disruption controller" logger="disruption-controller"
I1230 13:54:35.554552       1 shared_informer.go:349] "Waiting for caches to sync" controller="disruption"
I1230 13:54:35.559068       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1230 13:54:35.586120       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1230 13:54:35.589532       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1230 13:54:35.599647       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1230 13:54:35.599759       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1230 13:54:35.601924       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1230 13:54:35.602752       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1230 13:54:35.604862       1 shared_informer.go:356] "Caches are synced" controller="node"
I1230 13:54:35.605029       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1230 13:54:35.605117       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1230 13:54:35.605139       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1230 13:54:35.605146       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1230 13:54:35.606685       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1230 13:54:35.607480       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1230 13:54:35.608787       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1230 13:54:35.612516       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1230 13:54:35.613488       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1230 13:54:35.615232       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1230 13:54:35.616473       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1230 13:54:35.619481       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1230 13:54:35.619538       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1230 13:54:35.619551       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1230 13:54:35.619558       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1230 13:54:35.651546       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1230 13:54:35.652453       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1230 13:54:35.652549       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1230 13:54:35.652581       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1230 13:54:35.652641       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1230 13:54:35.654501       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1230 13:54:35.654511       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1230 13:54:35.654534       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1230 13:54:35.655457       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1230 13:54:35.655472       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1230 13:54:35.665507       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1230 13:54:35.668489       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1230 13:54:35.678502       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1230 13:54:35.684565       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1230 13:54:35.686562       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1230 13:54:35.686709       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1230 13:54:35.686877       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1230 13:54:35.687010       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1230 13:54:35.687983       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1230 13:54:35.691562       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1230 13:54:35.694526       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1230 13:54:35.697764       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1230 13:54:35.701698       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1230 13:54:35.702599       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1230 13:54:35.703615       1 shared_informer.go:356] "Caches are synced" controller="job"
I1230 13:54:35.703794       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1230 13:54:36.659972       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1230 13:54:36.676642       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1230 13:54:36.682652       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1230 13:54:36.682703       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1230 13:54:36.682717       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I1230 13:54:36.700763       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"


==> kube-proxy [aa113a4051a7] <==
I1230 13:36:31.275960       1 server_linux.go:53] "Using iptables proxy"
I1230 13:36:31.359173       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1230 13:36:33.059760       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1230 13:36:33.059794       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1230 13:36:33.059857       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1230 13:36:33.084718       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1230 13:36:33.084793       1 server_linux.go:132] "Using iptables Proxier"
I1230 13:36:33.090236       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1230 13:36:33.090495       1 server.go:527] "Version info" version="v1.34.0"
I1230 13:36:33.090510       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1230 13:36:33.091590       1 config.go:200] "Starting service config controller"
I1230 13:36:33.091605       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1230 13:36:33.091620       1 config.go:106] "Starting endpoint slice config controller"
I1230 13:36:33.091624       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1230 13:36:33.091632       1 config.go:403] "Starting serviceCIDR config controller"
I1230 13:36:33.091636       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1230 13:36:33.091723       1 config.go:309] "Starting node config controller"
I1230 13:36:33.091830       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1230 13:36:33.191873       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1230 13:36:33.191894       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1230 13:36:33.191910       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1230 13:36:33.191901       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [d94e3001d887] <==
I1230 13:54:33.394576       1 server_linux.go:53] "Using iptables proxy"
I1230 13:54:33.443656       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1230 13:54:33.545516       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1230 13:54:33.545563       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1230 13:54:33.545634       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1230 13:54:33.568684       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1230 13:54:33.568722       1 server_linux.go:132] "Using iptables Proxier"
I1230 13:54:33.574233       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1230 13:54:33.575872       1 server.go:527] "Version info" version="v1.34.0"
I1230 13:54:33.575896       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1230 13:54:33.577004       1 config.go:200] "Starting service config controller"
I1230 13:54:33.577022       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1230 13:54:33.577038       1 config.go:106] "Starting endpoint slice config controller"
I1230 13:54:33.577041       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1230 13:54:33.577066       1 config.go:403] "Starting serviceCIDR config controller"
I1230 13:54:33.577071       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1230 13:54:33.577200       1 config.go:309] "Starting node config controller"
I1230 13:54:33.577224       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1230 13:54:33.677542       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1230 13:54:33.677571       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I1230 13:54:33.677570       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1230 13:54:33.678481       1 shared_informer.go:356] "Caches are synced" controller="node config"


==> kube-scheduler [3c3ba821312f] <==
I1230 13:36:31.620973       1 serving.go:386] Generated self-signed cert in-memory
W1230 13:36:32.986342       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1230 13:36:32.986384       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1230 13:36:32.986396       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1230 13:36:32.986404       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1230 13:36:33.005136       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1230 13:36:33.005749       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1230 13:36:33.009832       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1230 13:36:33.009965       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1230 13:36:33.010155       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1230 13:36:33.009984       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1230 13:36:33.111827       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1230 13:53:29.671544       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1230 13:53:29.671563       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I1230 13:53:29.671581       1 server.go:263] "[graceful-termination] secure server has stopped listening"
I1230 13:53:29.671594       1 server.go:265] "[graceful-termination] secure server is exiting"
E1230 13:53:29.671608       1 run.go:72] "command failed" err="finished without leader elect"


==> kube-scheduler [f1d845665838] <==
I1230 13:54:31.469678       1 serving.go:386] Generated self-signed cert in-memory
W1230 13:54:32.252857       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1230 13:54:32.252888       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1230 13:54:32.252898       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1230 13:54:32.252905       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1230 13:54:32.268573       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1230 13:54:32.268607       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1230 13:54:32.271281       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1230 13:54:32.271439       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1230 13:54:32.271469       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1230 13:54:32.271505       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1230 13:54:32.372502       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Dec 30 13:54:32 minikube kubelet[1447]: E1230 13:54:32.552658    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:54:33.052612634 +0000 UTC m=+2.836752116 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.958207    1447 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0ce4a389ac4cd94e6abae3470d4aa5272737e1f60163c6ae5425f7abc51e40bc"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.970674    1447 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="da21a57fc8f12f1d452fac37b5c64fb5a1da4ab58d4791605e12c4cc647c981b"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.984265    1447 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="070f97871c555880f1b2560566bd7cbd0c811e21f09cbc1c0f816b209a9ce3f3"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.998132    1447 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="34cdab0c24d44b17557c1b984243c48dc069c817b044475f0eb69433cc3fd365"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.998231    1447 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.998610    1447 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.998933    1447 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Dec 30 13:54:32 minikube kubelet[1447]: I1230 13:54:32.999252    1447 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Dec 30 13:54:33 minikube kubelet[1447]: E1230 13:54:33.030495    1447 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Dec 30 13:54:33 minikube kubelet[1447]: E1230 13:54:33.032056    1447 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Dec 30 13:54:33 minikube kubelet[1447]: E1230 13:54:33.032250    1447 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Dec 30 13:54:33 minikube kubelet[1447]: E1230 13:54:33.032322    1447 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Dec 30 13:54:33 minikube kubelet[1447]: E1230 13:54:33.056715    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:54:34.056696275 +0000 UTC m=+3.840835695 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:54:34 minikube kubelet[1447]: E1230 13:54:34.065709    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:54:36.065689642 +0000 UTC m=+5.849829023 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:54:35 minikube kubelet[1447]: I1230 13:54:35.348451    1447 csi_plugin.go:106] kubernetes.io/csi: Trying to validate a new CSI Driver with name: hostpath.csi.k8s.io endpoint: /var/lib/kubelet/plugins/csi-hostpath/csi.sock versions: 1.0.0
Dec 30 13:54:35 minikube kubelet[1447]: I1230 13:54:35.348519    1447 csi_plugin.go:119] kubernetes.io/csi: Register new plugin with name: hostpath.csi.k8s.io at endpoint: /var/lib/kubelet/plugins/csi-hostpath/csi.sock
Dec 30 13:54:36 minikube kubelet[1447]: E1230 13:54:36.092173    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:54:40.09210865 +0000 UTC m=+9.876248721 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:54:36 minikube kubelet[1447]: I1230 13:54:36.529103    1447 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Dec 30 13:54:40 minikube kubelet[1447]: E1230 13:54:40.146705    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:54:48.14668703 +0000 UTC m=+17.930826416 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:54:41 minikube kubelet[1447]: I1230 13:54:41.664135    1447 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Dec 30 13:54:48 minikube kubelet[1447]: E1230 13:54:48.240903    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:55:04.240855492 +0000 UTC m=+34.025237131 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.898881    1447 scope.go:117] "RemoveContainer" containerID="e0943860deb213b9e2c7168e4b65a9f1f2719d58c5be04406e1a7356630e39c8"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.899335    1447 scope.go:117] "RemoveContainer" containerID="55b6deb657166c8a0c443994196f244162e58123402ed17a0ecb5e8069c7f604"
Dec 30 13:55:03 minikube kubelet[1447]: E1230 13:55:03.899631    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(e21392fb-e8fd-4c01-8cb4-b51288b6a659)\"" pod="kube-system/storage-provisioner" podUID="e21392fb-e8fd-4c01-8cb4-b51288b6a659"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.913207    1447 scope.go:117] "RemoveContainer" containerID="27cb8fc5149d9015a2612cfc3a3d8b1f59c99ee5f799ab1dedb8cd45ac5de776"
Dec 30 13:55:03 minikube kubelet[1447]: E1230 13:55:03.913408    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-disk-exporter\" with CrashLoopBackOff: \"back-off 10s restarting failed container=node-disk-exporter pod=openebs-ndm-node-exporter-7zqb8_openebs(108c59bf-bd29-44d8-a584-83778be37a6f)\"" pod="openebs/openebs-ndm-node-exporter-7zqb8" podUID="108c59bf-bd29-44d8-a584-83778be37a6f"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.924847    1447 scope.go:117] "RemoveContainer" containerID="d0b55a6cc2d5bca0355a178c0da413274e4f9f11ce4ab542f8ada2b10ef7d500"
Dec 30 13:55:03 minikube kubelet[1447]: E1230 13:55:03.925004    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"node-disk-operator\" with CrashLoopBackOff: \"back-off 10s restarting failed container=node-disk-operator pod=openebs-ndm-operator-f86d5ccd6-gnx9t_openebs(0e166734-a831-4afe-829b-37431a81efef)\"" pod="openebs/openebs-ndm-operator-f86d5ccd6-gnx9t" podUID="0e166734-a831-4afe-829b-37431a81efef"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.926046    1447 scope.go:117] "RemoveContainer" containerID="2f6c332c2f89a96b231c17baf615000d41cfd68d096a5373588ed7f7849f9c2f"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.940287    1447 scope.go:117] "RemoveContainer" containerID="904cdc63b2f07e94b38fa1370314d4f641f08e87ace869f2d5a207d7d6a6a0c1"
Dec 30 13:55:03 minikube kubelet[1447]: E1230 13:55:03.940408    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"ndm-cluster-exporter\" with CrashLoopBackOff: \"back-off 10s restarting failed container=ndm-cluster-exporter pod=openebs-ndm-cluster-exporter-556fc4c74d-c5fsk_openebs(a0e7f981-19f6-4b37-96b6-eadb701fef76)\"" pod="openebs/openebs-ndm-cluster-exporter-556fc4c74d-c5fsk" podUID="a0e7f981-19f6-4b37-96b6-eadb701fef76"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.942307    1447 scope.go:117] "RemoveContainer" containerID="d0c412e8c53106b2cd38e8599253a03652f0746083b5b7d30af64a944b998917"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.950226    1447 scope.go:117] "RemoveContainer" containerID="ac86a2cfc1b4545eb42567fef15e16114cbe607fa8461a6b4ea0dd584e6ce317"
Dec 30 13:55:03 minikube kubelet[1447]: E1230 13:55:03.950388    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"openebs-provisioner-hostpath\" with CrashLoopBackOff: \"back-off 10s restarting failed container=openebs-provisioner-hostpath pod=openebs-localpv-provisioner-5d77988647-5jrdt_openebs(b1039946-bf52-41f1-9322-aebb04ee1c03)\"" pod="openebs/openebs-localpv-provisioner-5d77988647-5jrdt" podUID="b1039946-bf52-41f1-9322-aebb04ee1c03"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.960503    1447 scope.go:117] "RemoveContainer" containerID="3d545feed3cd42bcc4199a837d45325b8b2c7a4aeb6669dc4fad7d94831e6ed1"
Dec 30 13:55:03 minikube kubelet[1447]: I1230 13:55:03.983302    1447 scope.go:117] "RemoveContainer" containerID="db3b8a0e9f767648a8c0ee4d837338b34df0ddfe356cb516534d932d241d606d"
Dec 30 13:55:04 minikube kubelet[1447]: E1230 13:55:04.342418    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:55:36.342358596 +0000 UTC m=+66.126740564 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:55:15 minikube kubelet[1447]: I1230 13:55:15.299349    1447 scope.go:117] "RemoveContainer" containerID="55b6deb657166c8a0c443994196f244162e58123402ed17a0ecb5e8069c7f604"
Dec 30 13:55:15 minikube kubelet[1447]: I1230 13:55:15.299848    1447 scope.go:117] "RemoveContainer" containerID="ac86a2cfc1b4545eb42567fef15e16114cbe607fa8461a6b4ea0dd584e6ce317"
Dec 30 13:55:15 minikube kubelet[1447]: I1230 13:55:15.301272    1447 scope.go:117] "RemoveContainer" containerID="d0b55a6cc2d5bca0355a178c0da413274e4f9f11ce4ab542f8ada2b10ef7d500"
Dec 30 13:55:15 minikube kubelet[1447]: I1230 13:55:15.301892    1447 scope.go:117] "RemoveContainer" containerID="27cb8fc5149d9015a2612cfc3a3d8b1f59c99ee5f799ab1dedb8cd45ac5de776"
Dec 30 13:55:15 minikube kubelet[1447]: I1230 13:55:15.302842    1447 scope.go:117] "RemoveContainer" containerID="904cdc63b2f07e94b38fa1370314d4f641f08e87ace869f2d5a207d7d6a6a0c1"
Dec 30 13:55:36 minikube kubelet[1447]: E1230 13:55:36.440503    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:56:40.440429616 +0000 UTC m=+130.224124753 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:56:35 minikube kubelet[1447]: E1230 13:56:35.390777    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[udev], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="openebs/openebs-ndm-hjwj8" podUID="8a9cd0b2-f76c-4a45-9590-d3f62fa40908"
Dec 30 13:56:40 minikube kubelet[1447]: E1230 13:56:40.512127    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 13:58:42.512093375 +0000 UTC m=+252.295690844 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:57:27 minikube kubelet[1447]: I1230 13:57:27.314532    1447 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-c7vgb\" (UniqueName: \"kubernetes.io/projected/c9c03535-2145-44d9-8ff6-e02852c21991-kube-api-access-c7vgb\") pod \"c9c03535-2145-44d9-8ff6-e02852c21991\" (UID: \"c9c03535-2145-44d9-8ff6-e02852c21991\") "
Dec 30 13:57:27 minikube kubelet[1447]: I1230 13:57:27.321511    1447 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c9c03535-2145-44d9-8ff6-e02852c21991-kube-api-access-c7vgb" (OuterVolumeSpecName: "kube-api-access-c7vgb") pod "c9c03535-2145-44d9-8ff6-e02852c21991" (UID: "c9c03535-2145-44d9-8ff6-e02852c21991"). InnerVolumeSpecName "kube-api-access-c7vgb". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Dec 30 13:57:27 minikube kubelet[1447]: I1230 13:57:27.415904    1447 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-c7vgb\" (UniqueName: \"kubernetes.io/projected/c9c03535-2145-44d9-8ff6-e02852c21991-kube-api-access-c7vgb\") on node \"minikube\" DevicePath \"\""
Dec 30 13:57:28 minikube kubelet[1447]: I1230 13:57:28.038598    1447 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="da21a57fc8f12f1d452fac37b5c64fb5a1da4ab58d4791605e12c4cc647c981b"
Dec 30 13:57:34 minikube kubelet[1447]: I1230 13:57:34.401072    1447 reconciler_common.go:163] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zvqvh\" (UniqueName: \"kubernetes.io/projected/dbd8c51b-2d9a-4086-ba99-4e0e39d2068f-kube-api-access-zvqvh\") pod \"dbd8c51b-2d9a-4086-ba99-4e0e39d2068f\" (UID: \"dbd8c51b-2d9a-4086-ba99-4e0e39d2068f\") "
Dec 30 13:57:34 minikube kubelet[1447]: I1230 13:57:34.407726    1447 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/dbd8c51b-2d9a-4086-ba99-4e0e39d2068f-kube-api-access-zvqvh" (OuterVolumeSpecName: "kube-api-access-zvqvh") pod "dbd8c51b-2d9a-4086-ba99-4e0e39d2068f" (UID: "dbd8c51b-2d9a-4086-ba99-4e0e39d2068f"). InnerVolumeSpecName "kube-api-access-zvqvh". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Dec 30 13:57:34 minikube kubelet[1447]: I1230 13:57:34.503524    1447 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-zvqvh\" (UniqueName: \"kubernetes.io/projected/dbd8c51b-2d9a-4086-ba99-4e0e39d2068f-kube-api-access-zvqvh\") on node \"minikube\" DevicePath \"\""
Dec 30 13:57:35 minikube kubelet[1447]: I1230 13:57:35.207410    1447 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="0ce4a389ac4cd94e6abae3470d4aa5272737e1f60163c6ae5425f7abc51e40bc"
Dec 30 13:58:42 minikube kubelet[1447]: E1230 13:58:42.515379    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 14:00:44.515342675 +0000 UTC m=+374.298148864 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 13:58:52 minikube kubelet[1447]: E1230 13:58:52.302785    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[udev], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="openebs/openebs-ndm-hjwj8" podUID="8a9cd0b2-f76c-4a45-9590-d3f62fa40908"
Dec 30 14:00:44 minikube kubelet[1447]: E1230 14:00:44.519994    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 14:02:46.519975852 +0000 UTC m=+496.301908099 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 14:01:06 minikube kubelet[1447]: E1230 14:01:06.304673    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[udev], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="openebs/openebs-ndm-hjwj8" podUID="8a9cd0b2-f76c-4a45-9590-d3f62fa40908"
Dec 30 14:02:46 minikube kubelet[1447]: E1230 14:02:46.585534    1447 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev podName:8a9cd0b2-f76c-4a45-9590-d3f62fa40908 nodeName:}" failed. No retries permitted until 2025-12-30 14:04:48.585409271 +0000 UTC m=+618.369509440 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "udev" (UniqueName: "kubernetes.io/host-path/8a9cd0b2-f76c-4a45-9590-d3f62fa40908-udev") pod "openebs-ndm-hjwj8" (UID: "8a9cd0b2-f76c-4a45-9590-d3f62fa40908") : hostPath type check failed: /run/udev is not a directory
Dec 30 14:03:21 minikube kubelet[1447]: E1230 14:03:21.297861    1447 pod_workers.go:1324] "Error syncing pod, skipping" err="unmounted volumes=[udev], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="openebs/openebs-ndm-hjwj8" podUID="8a9cd0b2-f76c-4a45-9590-d3f62fa40908"


==> storage-provisioner [037035361ccc] <==
W1230 14:02:35.700835       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:35.713806       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:37.716753       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:37.731724       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:39.739782       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:39.758938       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:41.761514       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:41.775550       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:43.778434       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:43.786010       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:45.788352       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:45.796638       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:47.798746       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:47.823805       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:49.829703       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:49.847224       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:51.851567       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:51.869123       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:53.875783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:53.895379       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:55.900904       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:55.916901       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:57.922518       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:57.946025       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:59.949139       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:02:59.966033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:01.968559       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:01.973549       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:03.980909       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:04.000633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:06.006966       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:06.024627       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:08.027835       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:08.037636       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:10.043585       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:10.052990       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:12.059344       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:12.075796       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:14.081399       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:14.099931       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:16.107588       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:16.127511       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:18.133026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:18.158644       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:20.164210       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:20.183835       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:22.189915       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:22.198584       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:24.204115       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:24.222334       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:26.229517       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:26.249917       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:28.254318       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:28.274508       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:30.279110       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:30.285439       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:32.289225       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:32.297621       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:34.304842       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1230 14:03:34.318631       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [55b6deb65716] <==
I1230 13:54:33.304639       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1230 13:55:03.309602       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

